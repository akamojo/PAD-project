{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part2_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akamojo/PAD-project/blob/master/part2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G_3rXJgHjus",
        "colab_type": "text"
      },
      "source": [
        "# PAD 2nd Assignment PART 2\n",
        "- Mitchell Galvao MIEI 41646\n",
        "- Urszula WaliÅ„ska MIEI 56556\n",
        "\n",
        "## Automatic Extraction of Explicit and Implicit Keyword  \n",
        "In our method, we first extract Relevant Expressions from a set of several documents, using \n",
        "LocalMaxs  extractor  we  have  implemented before. In our solution, we use TF-IDF measure as a criteria to select the most informative RE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zx9U5s-q36iQ",
        "outputId": "61dc517c-636d-40ed-955f-c136040248a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[?25l  Downloading https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[K     | 768kB 1.2MB/s\n",
            "\u001b[?25hBuilding wheels for collected packages: tqdm\n",
            "  Building wheel for tqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-evzohiv4/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.28.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FfGi6IV3wHcr",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rffrtUAUwJR-",
        "outputId": "4492730a-1800-40f0-827f-964925da2e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "17-cUaXGwHc1",
        "colab": {}
      },
      "source": [
        "file = open(\"/content/drive/My Drive/STUDIA/SEM 8/pad/en1.8m.txt\", \"r\", encoding=\"utf8\")\n",
        "# file = open(\"en1.8m.txt\", \"r\", encoding=\"utf8\")\n",
        "\n",
        "whole_text = file.read()\n",
        "text = whole_text.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9PHsE2zqwHdE",
        "colab": {}
      },
      "source": [
        "def append_space(text, pattern, left=True):\n",
        "    p = re.compile(\"[\" + pattern + \"]\")\n",
        "    \n",
        "    for i, m in enumerate(p.finditer(text)):\n",
        "        if left:\n",
        "            text = text[: m.start() + i] + \" \" + text[m.start() + i :]\n",
        "        else:\n",
        "            text = text[: m.start() + 1 + i] + \" \" + text[m.start() + 1 + i :]\n",
        "            \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0oVrjurXwHdQ",
        "colab": {}
      },
      "source": [
        "def preprocessing(text):\n",
        "    for i in range(len(text)):\n",
        "        text[i] = append_space(text[i], \",\\])>\")\n",
        "        text[i] = append_space(text[i], \".\")\n",
        "        text[i] = append_space(text[i], \"\\[(<\", False)\n",
        "      \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9x-KrFiNwHdU",
        "colab": {}
      },
      "source": [
        "def build_n_gram(text_arr, n_gram, grams):\n",
        "    cur_gram = []\n",
        "    count_gram = 0\n",
        "\n",
        "    for i in range(len(text_arr)):\n",
        "\n",
        "        cur_gram = [text_arr[i]]\n",
        "\n",
        "        for j in range(i + 1, i + n_gram):\n",
        "            if j < len(text_arr):\n",
        "                cur_gram.append(text_arr[j])\n",
        "\n",
        "        if len(cur_gram) == n_gram:\n",
        "            try:\n",
        "                grams[\" \".join(cur_gram)] += 1\n",
        "            except KeyError:\n",
        "                grams[\" \".join(cur_gram)] = 1\n",
        "\n",
        "    return grams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pkr_cvxVwHdd",
        "colab": {}
      },
      "source": [
        "def build_dict(text, n_s):\n",
        "    dictionary = []\n",
        "    for n in range(n_s):\n",
        "        dictionary.append(dict())\n",
        "    \n",
        "    for n in range(n_s):\n",
        "        for line in text:\n",
        "            dictionary[n] = build_n_gram(line.split(), n + 1, dictionary[n])\n",
        "        \n",
        "    return dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZrO6W8pwHdg",
        "colab": {}
      },
      "source": [
        "def SCP_f(words, dictionary):\n",
        "    words_arr = words.split()\n",
        "    \n",
        "    numerator = dictionary[len(words_arr) - 1][words] ** 2\n",
        "    F = 1 / (len(words_arr) - 1)\n",
        "    denominator = 0\n",
        "    \n",
        "    for i in range(len(words_arr) - 1):\n",
        "        left = words_arr[0:i+1]\n",
        "        right = words_arr[i+1:]\n",
        "        \n",
        "        denominator += dictionary[len(left) - 1][\" \".join(left)] * dictionary[len(right) - 1][\" \".join(right)]\n",
        "        \n",
        "    F *= denominator\n",
        "    return numerator / F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bs4Uhmut6fSt",
        "colab": {}
      },
      "source": [
        "def dice(words, dictionary):\n",
        "    words_arr = words.split()\n",
        "    \n",
        "    numerator = dictionary[len(words_arr) - 1][words] * 2\n",
        "    F = 1 / (len(words_arr) - 1)\n",
        "    denominator = 0\n",
        "    \n",
        "    for i in range(len(words_arr) - 1):\n",
        "        left = words_arr[0:i+1]\n",
        "        right = words_arr[i+1:]\n",
        "        \n",
        "        denominator += dictionary[len(left) - 1][\" \".join(left)] + dictionary[len(right) - 1][\" \".join(right)]\n",
        "        \n",
        "    F *= denominator\n",
        "    return numerator / F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QFt8PTVOESoI",
        "colab": {}
      },
      "source": [
        "def LocalMax(text, table):\n",
        "    xys = dict()\n",
        "    \n",
        "#     for nr in tqdm(range(len(text))):\n",
        "    for nr in range(len(text)):\n",
        "        line = text[nr]\n",
        "\n",
        "        text_arr = line.split()\n",
        "\n",
        "        if len(text_arr) < 2:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(text_arr) - 2):\n",
        "            cur_gram = \" \".join(text_arr[i:i+2])\n",
        "\n",
        "            j = i + 3\n",
        "            n_gram = 1\n",
        "\n",
        "            while j < len(text_arr) and n_gram < 6:\n",
        "                next_gram = \" \".join(text_arr[i:j])\n",
        "\n",
        "                try:\n",
        "                    xys[cur_gram][1] = max(xys[cur_gram][1], table[next_gram])\n",
        "                    \n",
        "                except KeyError:\n",
        "                    if len(cur_gram.split()) > 2:\n",
        "                        x = table[\" \".join(cur_gram.split()[:-1])]\n",
        "                        x = max(x, table[\" \".join(cur_gram.split()[1:])])\n",
        "                        xys[cur_gram] = [x, table[next_gram]]\n",
        "                    else:\n",
        "                        xys[cur_gram] = [-1, table[next_gram]]\n",
        "\n",
        "                cur_gram = next_gram\n",
        "\n",
        "                j += 1\n",
        "                n_gram += 1\n",
        "\n",
        "            cur_gram = \" \".join(text_arr[i:i+2])\n",
        "            j = i - 1\n",
        "            n_gram = 1\n",
        "\n",
        "            while j >= 0 and n_gram < 6:\n",
        "                next_gram = \" \".join(text_arr[j:i+2])\n",
        "\n",
        "\n",
        "                try:                        \n",
        "                    xys[cur_gram][1] = max(xys[cur_gram][1], table[next_gram])\n",
        "                    \n",
        "                except KeyError:\n",
        "                    if len(cur_gram.split()) > 2:\n",
        "                        x = table[\" \".join(cur_gram.split()[:-1])]\n",
        "                        x = max(x, table[\" \".join(cur_gram.split()[1:])])\n",
        "                        xys[cur_gram] = [x, table[next_gram]]\n",
        "                    else:\n",
        "                        xys[cur_gram] = [-1, table[next_gram]]\n",
        "\n",
        "\n",
        "                cur_gram = next_gram\n",
        "\n",
        "                j -= 1\n",
        "                n_gram += 1\n",
        "              \n",
        "    return xys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0wd9fUsowHdq",
        "colab": {}
      },
      "source": [
        "def main(text, max_gram, threshold, glue_fun, verbose=False):\n",
        "    length = 0\n",
        "    for line in text:\n",
        "        length += len(line.split())\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Preprocessing text\")\n",
        "    text = preprocessing(text)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Building dictionary with frequencies\")\n",
        "\n",
        "    dictionary = build_dict(text, max_gram)\n",
        "    \n",
        "    table = dict()\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Proceeding to calculating glue value\")\n",
        "        print(\"Glue value calculated for:\")\n",
        "\n",
        "    for i in range(2, max_gram + 1):\n",
        "        if verbose:\n",
        "            print(i, \"grams\")\n",
        "            \n",
        "        for s in dictionary[i - 1]:\n",
        "            table[s] = glue_fun(s, dictionary)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Proceeding to Localmax\")\n",
        "    \n",
        "    RE = dict()\n",
        "    xys = LocalMax(text, table)\n",
        "            \n",
        "    for g in xys:\n",
        "        if xys[g][0] == -1:\n",
        "            xys[g][0] = xys[g][1]\n",
        "        \n",
        "        val = (xys[g][0] + xys[g][1]) / 2\n",
        "        \n",
        "        if table[g] > val and re.match(r\"^[a-z\\'A-Z\\s]*$\", g) and dictionary[len(g.split()) - 1][g] > threshold:\n",
        "            RE[g] = dictionary[len(g.split()) - 1][g] / (length / len(g.split()))\n",
        "    \n",
        "    return RE, dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8ClubNHIoAT",
        "colab_type": "text"
      },
      "source": [
        "This time, the list of obtainted Relevant Expressions that is returned contains also the term frequency (TF) that will be used when extracting Explict Keywords from documents. The dictionary which contains frequencies of each n-gram (up to 7-gram) is returned as well. It will be useful when calculating the IDF value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0MGuj-XKEl89",
        "outputId": "3b9fb822-2345-4444-c187-16bce81fc4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "test = text[0]\n",
        "\n",
        "RE, dictionary = main([test], 7, 1, SCP_f)\n",
        "print(\"Obtained\", len(RE), \"Relevant Expressions\")\n",
        "RE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtained 2 Relevant Expressions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'John Philoponus': 0.0380952380952381, 'in the': 0.0761904761904762}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szLGd6poL1zT",
        "colab_type": "text"
      },
      "source": [
        "Effect of applying method to the first paragraph of the corpus. As we can see, the TF value for 'in the' gram is higher than 'John Philoponus' however, we will notice later that taking into account also IDF value will discriminate such irrelevant expressions as 'in the' while extracting Explicit Keywords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D_H1A0vOBBg0",
        "colab": {}
      },
      "source": [
        "def split_docs(whole_text):\n",
        "    docs1 = whole_text.split('</doc')\n",
        "#     docs = []\n",
        "#     for d in docs1:\n",
        "#         docs.append(d.split('<doc'))\n",
        "        \n",
        "#     return [item for sublist in docs for item in sublist]\n",
        "    return docs1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XOBMFYeyjYf",
        "colab_type": "code",
        "outputId": "a60489bb-57bc-42c4-a216-23748a6666ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import math\n",
        "\n",
        "docs = split_docs(whole_text)\n",
        "# n = len(docs)\n",
        "n = 1000\n",
        "REs = []\n",
        "word_count = dict()\n",
        "\n",
        "for i in tqdm(range(n)):\n",
        "    RE, dictionary = main(docs[i].split('\\n'), 7, 1, SCP_f)\n",
        "    REs.append(RE)\n",
        "    \n",
        "    for grams in dictionary:\n",
        "        for g in grams:\n",
        "            try:\n",
        "                word_count[g] += 1\n",
        "            except KeyError:\n",
        "                word_count[g] = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm_notebook.py:88: TqdmExperimentalWarning: Detect Google Colab 0.0.1a2 and thus load dummy ipywidgets package. Note that UI is different from that in Jupyter. See https://github.com/tqdm/tqdm/pull/640\n",
            "  \" See https://github.com/tqdm/tqdm/pull/640\".format(colab.__version__), TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='1000' value='1000'></progress>100% 1000/1000 [00:54&lt;00:00, 18.30it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7BebWNUGEcNm",
        "colab": {}
      },
      "source": [
        "ex_keywords = []\n",
        "\n",
        "for i in range(len(REs)):\n",
        "    ex_keywords.append(dict())\n",
        "    for r in REs[i]:\n",
        "        tf = REs[i][r]\n",
        "        idf = math.log10(n / float(word_count[r]))\n",
        "        ex_keywords[-1][r] = tf * idf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TGR_6lJMwmp",
        "colab_type": "text"
      },
      "source": [
        "## Results  \n",
        "Below the results of our method for the first 10 documents were presented. For each document a list of 10-15 most informative Relevant Expressions considered as the Explicit Keywords of the document were presented. They correspond to 10-15 Relevant Expressions having the highest TF-IDF value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bn-BqhSzIMZc",
        "outputId": "878737e6-f37e-4cd1-d8c5-72e56d7b36b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2773
        }
      },
      "source": [
        "print(\"--------------------------------------------------\")\n",
        "print(\"TOP 15 Explicit Keywords for 10 documents\")\n",
        "print(\"--------------------------------------------------\\n\")\n",
        "\n",
        "for i, l in enumerate(ex_keywords[:10]):\n",
        "    print(\"Doc\", i+1)\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    for elem, val in sorted(l.items(), key=lambda kv: -kv[1])[:15]:\n",
        "        print(elem, \":\\t\", val)\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "TOP 15 Explicit Keywords for documents\n",
            "--------------------------------------------------\n",
            "\n",
            "Doc 1\n",
            "--------------------------------------------------\n",
            "may be given to any :\t 0.00577478344562079\n",
            "depart for the Holy Land :\t 0.00577478344562079\n",
            "removal of a hydrogen ion :\t 0.00577478344562079\n",
            "Indic scripts :\t 0.0046198267564966315\n",
            "head of a monastery :\t 0.0046198267564966315\n",
            "depart for the Holy :\t 0.0046198267564966315\n",
            "below the national average :\t 0.0046198267564966315\n",
            "removal of a hydrogen :\t 0.0046198267564966315\n",
            "hydrogen ion :\t 0.0046198267564966315\n",
            "a monastery :\t 0.004156257947004456\n",
            "may be given :\t 0.0034648700673724736\n",
            "murder of Naboth :\t 0.0034648700673724736\n",
            "Peter of Courtenay :\t 0.0034648700673724736\n",
            "below the national :\t 0.0034648700673724736\n",
            "Louis the Child :\t 0.0034648700673724736\n",
            "\n",
            "\n",
            "Doc 2\n",
            "--------------------------------------------------\n",
            "by a cannonball :\t 0.021201413427561835\n",
            "an independent record :\t 0.021201413427561835\n",
            "football clubs :\t 0.014134275618374558\n",
            "black metal :\t 0.014134275618374558\n",
            "struck in :\t 0.01271599530900362\n",
            "Premier League :\t 0.010152659411004678\n",
            "other countries :\t 0.009049793893768551\n",
            "an independent :\t 0.006979910766181919\n",
            "\n",
            "\n",
            "Doc 3\n",
            "--------------------------------------------------\n",
            "Coronae Borealis :\t 0.008581644815256258\n",
            "Common Lisp :\t 0.007151370679380214\n",
            "formed a coalition with :\t 0.0057210965435041715\n",
            "orbit each other every :\t 0.0057210965435041715\n",
            "giant star of magnitude :\t 0.0057210965435041715\n",
            "Belfast Giants :\t 0.004290822407628129\n",
            "deposits of graphite :\t 0.004290822407628129\n",
            "formed a coalition :\t 0.004290822407628129\n",
            "Odyssey Arena :\t 0.0028605482717520858\n",
            "big bang :\t 0.0028605482717520858\n",
            "spoken natively :\t 0.0028605482717520858\n",
            "life expectancy :\t 0.0028605482717520858\n",
            "casuistic reasoning :\t 0.0028605482717520858\n",
            "real humans :\t 0.0028605482717520858\n",
            "Liberty Cup :\t 0.0028605482717520858\n",
            "\n",
            "\n",
            "Doc 4\n",
            "--------------------------------------------------\n",
            "chimp will :\t 0.03252032520325203\n",
            "from dub :\t 0.03252032520325203\n",
            "richest province :\t 0.03252032520325203\n",
            "facial expressions :\t 0.029257127418276626\n",
            "\n",
            "\n",
            "Doc 5\n",
            "--------------------------------------------------\n",
            "hop music and culture :\t 0.007897334649555774\n",
            "used as a unit :\t 0.007897334649555774\n",
            "rainfall amounts and intensities :\t 0.007897334649555774\n",
            "approximately the same :\t 0.00592300098716683\n",
            "Dar es Salaam :\t 0.00592300098716683\n",
            "music and culture :\t 0.00592300098716683\n",
            "floating point operations :\t 0.00592300098716683\n",
            "Canary Wharf :\t 0.00592300098716683\n",
            "encyclopedia articles :\t 0.00592300098716683\n",
            "amounts and intensities :\t 0.00592300098716683\n",
            "fees and fines :\t 0.00592300098716683\n",
            "vacuum tube :\t 0.005328667333338635\n",
            "had already been :\t 0.004254495478747765\n",
            "hop music :\t 0.003948667324777887\n",
            "Tel Dan :\t 0.003948667324777887\n",
            "\n",
            "\n",
            "Doc 6\n",
            "--------------------------------------------------\n",
            "formal languages :\t 0.024057738572574178\n",
            "formal languages are used :\t 0.019246190858059342\n",
            "no family names :\t 0.014434643143544507\n",
            "Newton's First Law :\t 0.014434643143544507\n",
            "constant velocity :\t 0.014434643143544507\n",
            "net force :\t 0.014434643143544507\n",
            "languages are used :\t 0.014434643143544507\n",
            "flag should :\t 0.009623095429029671\n",
            "which is now :\t 0.009242111085576784\n",
            "Ne Win :\t 0.008657481970604711\n",
            "\n",
            "\n",
            "Doc 7\n",
            "--------------------------------------------------\n",
            "of the Foreign Legion :\t 0.2857142857142857\n",
            "Foreign Legion :\t 0.2857142857142857\n",
            "of the Foreign :\t 0.19278357173828703\n",
            "\n",
            "\n",
            "Doc 8\n",
            "--------------------------------------------------\n",
            "kinetic energy of the mass :\t 0.011679211004412146\n",
            "speaking it is the area west :\t 0.009343368803529718\n",
            "energy of the mass :\t 0.009343368803529718\n",
            "proven reserves in the world :\t 0.007786140669608097\n",
            "speaking it is the area :\t 0.007786140669608097\n",
            "real algebraic numbers :\t 0.007007526602647288\n",
            "sequence of real :\t 0.007007526602647288\n",
            "Parliament of England :\t 0.007007526602647288\n",
            "real numbers :\t 0.007004853372271007\n",
            "reserves in the world :\t 0.0062289125356864775\n",
            "not in the sequence :\t 0.0062289125356864775\n",
            "largest in Africa :\t 0.004671684401764859\n",
            "Government of Ghana :\t 0.004671684401764859\n",
            "Lynne and Petty :\t 0.004671684401764859\n",
            "Walk of Fame :\t 0.004671684401764859\n",
            "\n",
            "\n",
            "Doc 9\n",
            "--------------------------------------------------\n",
            "steppe nomads and peoples :\t 0.007799805004874878\n",
            "Grand Master :\t 0.007799805004874878\n",
            "named in his honour :\t 0.007017146582609084\n",
            "intercalary years :\t 0.005849853753656159\n",
            "not that it's :\t 0.005849853753656158\n",
            "nomads and peoples :\t 0.005849853753656158\n",
            "may not cross :\t 0.005849853753656158\n",
            "Kent State :\t 0.005262859936956812\n",
            "Mexican history :\t 0.003899902502437439\n",
            "Anthony Quinn :\t 0.003899902502437439\n",
            "extended format :\t 0.003899902502437439\n",
            "Holyland Tower :\t 0.003899902502437439\n",
            "steppe nomads :\t 0.003899902502437439\n",
            "Asian Avars :\t 0.003899902502437439\n",
            "dog meat :\t 0.003899902502437439\n",
            "\n",
            "\n",
            "Doc 10\n",
            "--------------------------------------------------\n",
            "linear maps corresponds to the matrix :\t 0.020224719101123594\n",
            "maps corresponds to the matrix :\t 0.016853932584269662\n",
            "defective pixels :\t 0.01348314606741573\n",
            "corresponds to the matrix :\t 0.01348314606741573\n",
            "LCD panel :\t 0.010112359550561799\n",
            "linear maps :\t 0.010112359550561799\n",
            "what would become :\t 0.008504085658248327\n",
            "my mother :\t 0.006741573033707865\n",
            "trading estate :\t 0.006741573033707865\n",
            "SVGA LCD :\t 0.006741573033707865\n",
            "word pairs :\t 0.006741573033707865\n",
            "Lisp programmers :\t 0.006741573033707865\n",
            "situation could :\t 0.006065101133339369\n",
            "at home :\t 0.004316446638095225\n",
            "such as :\t 0.003097155506151237\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxVr03FGN0lv",
        "colab_type": "text"
      },
      "source": [
        "Without a shadow of a doubt, not every expression was classified correctly as Explicit Keyword. However, we can also notice quite a satisfying number of expressions that could possibly be considered as Explicit Keywords of the document, like for instance, 'Liberty Cup' or 'Newton's First Law'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RLSBBaV5t8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_doc = dict()\n",
        "\n",
        "for i in range(len(REs)):\n",
        "    for w in list(REs[i].keys()):\n",
        "        try:\n",
        "            word_doc[w].add(i)\n",
        "        except KeyError:\n",
        "            word_doc[w] = set()\n",
        "            word_doc[w].add(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVM7g59m74Cd",
        "colab_type": "code",
        "outputId": "5ff67e16-d9b8-47ca-96e3-5e99c61e1647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "im_keywords = dict()\n",
        "\n",
        "with tqdm(total=len(word_doc)) as pbar:\n",
        "    for w1 in word_doc:\n",
        "        for w2 in word_doc:\n",
        "            if w1 != w2:\n",
        "                try:\n",
        "                    count = len(set.intersection(word_doc[w1], word_doc[w2]))\n",
        "                    im_keywords[w1].append([w2, count])\n",
        "                except KeyError:\n",
        "                    im_keywords[w1] = []\n",
        "                    im_keywords[w1].append([w2, count])\n",
        "\n",
        "                try:\n",
        "                    im_keywords[w2].append([w1, count])\n",
        "                except KeyError:\n",
        "                    im_keywords[w2] = []\n",
        "                    im_keywords[w2].append([w1, count])\n",
        "                    \n",
        "        pbar.update(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='5887' value='5887'></progress>100% 5887/5887 [02:12&lt;00:00, 44.53it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ErB3lRvQK1",
        "colab_type": "code",
        "outputId": "551e5917-2fb5-4506-a72a-8a98a2b48ec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "sorted(im_keywords['in the'], key=lambda kv: -kv[1])[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['of the', 33],\n",
              " ['of the', 33],\n",
              " ['United States', 13],\n",
              " ['United States', 13],\n",
              " ['such as', 7]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-bnrVBZBkI0",
        "colab_type": "code",
        "outputId": "4a26f7f7-b707-4925-cc61-50046f1b2b6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "print(\"--------------------------------------------------\")\n",
        "print(\"TOP Implicit Keywords for document 1\")\n",
        "print(\"--------------------------------------------------\\n\")\n",
        "\n",
        "top0 = sorted(ex_keywords[100].items(), key=lambda kv: -kv[1])[:15]\n",
        "\n",
        "for k, v in top0:\n",
        "    print(k, v)\n",
        "    print(sorted(im_keywords[k], key=lambda kv: -kv[1])[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "TOP Implicit Keywords for document 1\n",
            "--------------------------------------------------\n",
            "\n",
            "over the speed of the tractor 0.02737642585551331\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1]]\n",
            "Kennet and Avon Canal 0.018250950570342206\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Avon Canal', 1]]\n",
            "control over the speed 0.018250950570342206\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1]]\n",
            "speed of the tractor 0.018250950570342206\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1]]\n",
            "Kennet and Avon 0.013688212927756654\n",
            "[['Great Bedwyn', 1], ['Great Bedwyn', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1]]\n",
            "they look at 0.013688212927756654\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1]]\n",
            "over the speed 0.013688212927756654\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1]]\n",
            "Great Bedwyn 0.009125475285171103\n",
            "[['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1], ['they look at', 1]]\n",
            "Avon Canal 0.009125475285171103\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Great Bedwyn', 1], ['Kennet and Avon', 1]]\n",
            "they look 0.009125475285171103\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['Great Bedwyn', 1]]\n",
            "Hells Angels 0.009125475285171103\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1]]\n",
            "helps provide 0.009125475285171103\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1]]\n",
            "look at 0.008764192033242777\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1]]\n",
            "control over 0.005548011379298308\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1]]\n",
            "This is 0.003320197661205628\n",
            "[['Great Bedwyn', 1], ['Kennet and Avon', 1], ['Kennet and Avon Canal', 1], ['Avon Canal', 1], ['they look', 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}