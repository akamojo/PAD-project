{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part2_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akamojo/PAD-project/blob/master/part2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zx9U5s-q36iQ",
        "outputId": "ead2dfda-5441-4d3c-9b83-c2a8bd3ff3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[?25l  Downloading https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[K     | 768kB 1.2MB/s\n",
            "\u001b[?25hBuilding wheels for collected packages: tqdm\n",
            "  Building wheel for tqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u5_uhcsq/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.28.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FfGi6IV3wHcr",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rffrtUAUwJR-",
        "outputId": "47251a48-d694-4fba-e8ec-8ad6e0adfd74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "17-cUaXGwHc1",
        "colab": {}
      },
      "source": [
        "file = open(\"/content/drive/My Drive/STUDIA/SEM 8/pad/en1.8m.txt\", \"r\", encoding=\"utf8\")\n",
        "# file = open(\"en1.8m.txt\", \"r\", encoding=\"utf8\")\n",
        "\n",
        "whole_text = file.read()\n",
        "text = whole_text.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9PHsE2zqwHdE",
        "colab": {}
      },
      "source": [
        "def append_space(text, pattern, left=True):\n",
        "    p = re.compile(\"[\" + pattern + \"]\")\n",
        "    \n",
        "    for i, m in enumerate(p.finditer(text)):\n",
        "        if left:\n",
        "            text = text[: m.start() + i] + \" \" + text[m.start() + i :]\n",
        "        else:\n",
        "            text = text[: m.start() + 1 + i] + \" \" + text[m.start() + 1 + i :]\n",
        "            \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0oVrjurXwHdQ",
        "colab": {}
      },
      "source": [
        "def preprocessing(text):\n",
        "    for i in range(len(text)):\n",
        "        text[i] = append_space(text[i], \",\\])>\")\n",
        "        text[i] = append_space(text[i], \".\")\n",
        "        text[i] = append_space(text[i], \"\\[(<\", False)\n",
        "      \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9x-KrFiNwHdU",
        "colab": {}
      },
      "source": [
        "def build_n_gram(text_arr, n_gram, grams):\n",
        "    cur_gram = []\n",
        "    count_gram = 0\n",
        "\n",
        "    for i in range(len(text_arr)):\n",
        "\n",
        "        cur_gram = [text_arr[i]]\n",
        "\n",
        "        for j in range(i + 1, i + n_gram):\n",
        "            if j < len(text_arr):\n",
        "                cur_gram.append(text_arr[j])\n",
        "\n",
        "        if len(cur_gram) == n_gram:\n",
        "            try:\n",
        "                grams[\" \".join(cur_gram)] += 1\n",
        "            except KeyError:\n",
        "                grams[\" \".join(cur_gram)] = 1\n",
        "\n",
        "    return grams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pkr_cvxVwHdd",
        "colab": {}
      },
      "source": [
        "def build_dict(text, n_s):\n",
        "    dictionary = []\n",
        "    for n in range(n_s):\n",
        "        dictionary.append(dict())\n",
        "    \n",
        "    for n in range(n_s):\n",
        "        for line in text:\n",
        "            dictionary[n] = build_n_gram(line.split(), n + 1, dictionary[n])\n",
        "        \n",
        "    return dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZrO6W8pwHdg",
        "colab": {}
      },
      "source": [
        "def SCP_f(words, dictionary):\n",
        "    words_arr = words.split()\n",
        "    \n",
        "    numerator = dictionary[len(words_arr) - 1][words] ** 2\n",
        "    F = 1 / (len(words_arr) - 1)\n",
        "    denominator = 0\n",
        "    \n",
        "    for i in range(len(words_arr) - 1):\n",
        "        left = words_arr[0:i+1]\n",
        "        right = words_arr[i+1:]\n",
        "        \n",
        "        denominator += dictionary[len(left) - 1][\" \".join(left)] * dictionary[len(right) - 1][\" \".join(right)]\n",
        "        \n",
        "    F *= denominator\n",
        "    return numerator / F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bs4Uhmut6fSt",
        "colab": {}
      },
      "source": [
        "def dice(words, dictionary):\n",
        "    words_arr = words.split()\n",
        "    \n",
        "    numerator = dictionary[len(words_arr) - 1][words] * 2\n",
        "    F = 1 / (len(words_arr) - 1)\n",
        "    denominator = 0\n",
        "    \n",
        "    for i in range(len(words_arr) - 1):\n",
        "        left = words_arr[0:i+1]\n",
        "        right = words_arr[i+1:]\n",
        "        \n",
        "        denominator += dictionary[len(left) - 1][\" \".join(left)] + dictionary[len(right) - 1][\" \".join(right)]\n",
        "        \n",
        "    F *= denominator\n",
        "    return numerator / F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QFt8PTVOESoI",
        "colab": {}
      },
      "source": [
        "def LocalMax(text, table):\n",
        "    xys = dict()\n",
        "    \n",
        "#     for nr in tqdm(range(len(text))):\n",
        "    for nr in range(len(text)):\n",
        "        line = text[nr]\n",
        "\n",
        "        text_arr = line.split()\n",
        "\n",
        "        if len(text_arr) < 2:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(text_arr) - 2):\n",
        "            cur_gram = \" \".join(text_arr[i:i+2])\n",
        "\n",
        "            j = i + 3\n",
        "            n_gram = 1\n",
        "\n",
        "            while j < len(text_arr) and n_gram < 6:\n",
        "                next_gram = \" \".join(text_arr[i:j])\n",
        "\n",
        "                try:\n",
        "                    xys[cur_gram][1] = max(xys[cur_gram][1], table[next_gram])\n",
        "                    \n",
        "                except KeyError:\n",
        "                    if len(cur_gram.split()) > 2:\n",
        "                        x = table[\" \".join(cur_gram.split()[:-1])]\n",
        "                        x = max(x, table[\" \".join(cur_gram.split()[1:])])\n",
        "                        xys[cur_gram] = [x, table[next_gram]]\n",
        "                    else:\n",
        "                        xys[cur_gram] = [-1, table[next_gram]]\n",
        "\n",
        "                cur_gram = next_gram\n",
        "\n",
        "                j += 1\n",
        "                n_gram += 1\n",
        "\n",
        "            cur_gram = \" \".join(text_arr[i:i+2])\n",
        "            j = i - 1\n",
        "            n_gram = 1\n",
        "\n",
        "            while j >= 0 and n_gram < 6:\n",
        "                next_gram = \" \".join(text_arr[j:i+2])\n",
        "\n",
        "\n",
        "                try:                        \n",
        "                    xys[cur_gram][1] = max(xys[cur_gram][1], table[next_gram])\n",
        "                    \n",
        "                except KeyError:\n",
        "                    if len(cur_gram.split()) > 2:\n",
        "                        x = table[\" \".join(cur_gram.split()[:-1])]\n",
        "                        x = max(x, table[\" \".join(cur_gram.split()[1:])])\n",
        "                        xys[cur_gram] = [x, table[next_gram]]\n",
        "                    else:\n",
        "                        xys[cur_gram] = [-1, table[next_gram]]\n",
        "\n",
        "\n",
        "                cur_gram = next_gram\n",
        "\n",
        "                j -= 1\n",
        "                n_gram += 1\n",
        "              \n",
        "    return xys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0wd9fUsowHdq",
        "colab": {}
      },
      "source": [
        "def main(text, max_gram, threshold, glue_fun, verbose=False):\n",
        "    length = 0\n",
        "    for line in text:\n",
        "        length += len(line.split())\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Preprocessing text\")\n",
        "    text = preprocessing(text)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Building dictionary with frequencies\")\n",
        "\n",
        "    dictionary = build_dict(text, max_gram)\n",
        "    \n",
        "    table = dict()\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Proceeding to calculating glue value\")\n",
        "        print(\"Glue value calculated for:\")\n",
        "\n",
        "    for i in range(2, max_gram + 1):\n",
        "        if verbose:\n",
        "            print(i, \"grams\")\n",
        "            \n",
        "        for s in dictionary[i - 1]:\n",
        "            table[s] = glue_fun(s, dictionary)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Proceeding to Localmax\")\n",
        "    \n",
        "    RE = dict()\n",
        "    xys = LocalMax(text, table)\n",
        "            \n",
        "    for g in xys:\n",
        "        if xys[g][0] == -1:\n",
        "            xys[g][0] = xys[g][1]\n",
        "        \n",
        "        val = (xys[g][0] + xys[g][1]) / 2\n",
        "        \n",
        "        if table[g] > val and re.match(r\"^[a-z\\'A-Z\\s]*$\", g) and dictionary[len(g.split()) - 1][g] > threshold:\n",
        "            RE[g] = dictionary[len(g.split()) - 1][g] / (length / len(g.split()))\n",
        "    \n",
        "    return RE, dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0MGuj-XKEl89",
        "outputId": "69b8ddc4-add3-4e90-e57e-20130441596e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "test = text[0]\n",
        "\n",
        "RE, dictionary = main([test], 7, 1, SCP_f)\n",
        "print(\"Obtained\", len(RE), \"Relevant Expressions\")\n",
        "RE"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtained 2 Relevant Expressions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'John Philoponus': 0.0380952380952381, 'in the': 0.0761904761904762}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D_H1A0vOBBg0",
        "colab": {}
      },
      "source": [
        "def split_docs(whole_text):\n",
        "    docs1 = whole_text.split('</doc')\n",
        "#     docs = []\n",
        "#     for d in docs1:\n",
        "#         docs.append(d.split('<doc'))\n",
        "        \n",
        "#     return [item for sublist in docs for item in sublist]\n",
        "    return docs1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XOBMFYeyjYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "86a33bba-b60d-428e-9d63-4c8af5f16347"
      },
      "source": [
        "import math\n",
        "\n",
        "docs = split_docs(whole_text)\n",
        "n = 1000\n",
        "REs = []\n",
        "word_count = dict()\n",
        "\n",
        "for i in tqdm(range(n)):\n",
        "    RE, dictionary = main(docs[i].split('\\n'), 7, 1, SCP_f)\n",
        "    REs.append(RE)\n",
        "    \n",
        "    for grams in dictionary:\n",
        "        for g in grams:\n",
        "            try:\n",
        "                word_count[g] += 1\n",
        "            except KeyError:\n",
        "                word_count[g] = 1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm_notebook.py:88: TqdmExperimentalWarning: Detect Google Colab 0.0.1a2 and thus load dummy ipywidgets package. Note that UI is different from that in Jupyter. See https://github.com/tqdm/tqdm/pull/640\n",
            "  \" See https://github.com/tqdm/tqdm/pull/640\".format(colab.__version__), TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='1000' value='1000'></progress>100% 1000/1000 [00:52&lt;00:00, 19.05it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7BebWNUGEcNm",
        "colab": {}
      },
      "source": [
        "ex_keywords = []\n",
        "\n",
        "for i in range(len(REs)):\n",
        "    ex_keywords.append(dict())\n",
        "    for r in REs[i]:\n",
        "        tf = REs[i][r]\n",
        "        idf = math.log10(n / float(word_count[r]))\n",
        "        ex_keywords[-1][r] = tf * idf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bn-BqhSzIMZc",
        "outputId": "f3d30a81-debd-498e-a9f5-2bc5045bf2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2773
        }
      },
      "source": [
        "print(\"--------------------------------------------------\")\n",
        "print(\"TOP 15 Explicit Keywords for documents\")\n",
        "print(\"--------------------------------------------------\\n\")\n",
        "\n",
        "for i, l in enumerate(ex_keywords[:10]):\n",
        "    print(\"Doc\", i+1)\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    for elem, val in sorted(l.items(), key=lambda kv: -kv[1])[:15]:\n",
        "        print(elem, \":\\t\", val)\n",
        "    print('\\n')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "TOP 15 Explicit Keywords for documents\n",
            "--------------------------------------------------\n",
            "\n",
            "Doc 1\n",
            "--------------------------------------------------\n",
            "may be given to any :\t 0.00577478344562079\n",
            "depart for the Holy Land :\t 0.00577478344562079\n",
            "removal of a hydrogen ion :\t 0.00577478344562079\n",
            "Indic scripts :\t 0.0046198267564966315\n",
            "head of a monastery :\t 0.0046198267564966315\n",
            "depart for the Holy :\t 0.0046198267564966315\n",
            "below the national average :\t 0.0046198267564966315\n",
            "removal of a hydrogen :\t 0.0046198267564966315\n",
            "hydrogen ion :\t 0.0046198267564966315\n",
            "a monastery :\t 0.004156257947004456\n",
            "may be given :\t 0.0034648700673724736\n",
            "murder of Naboth :\t 0.0034648700673724736\n",
            "Peter of Courtenay :\t 0.0034648700673724736\n",
            "below the national :\t 0.0034648700673724736\n",
            "Louis the Child :\t 0.0034648700673724736\n",
            "\n",
            "\n",
            "Doc 2\n",
            "--------------------------------------------------\n",
            "by a cannonball :\t 0.021201413427561835\n",
            "an independent record :\t 0.021201413427561835\n",
            "football clubs :\t 0.014134275618374558\n",
            "black metal :\t 0.014134275618374558\n",
            "struck in :\t 0.01271599530900362\n",
            "Premier League :\t 0.010152659411004678\n",
            "other countries :\t 0.009049793893768551\n",
            "an independent :\t 0.006979910766181919\n",
            "\n",
            "\n",
            "Doc 3\n",
            "--------------------------------------------------\n",
            "Coronae Borealis :\t 0.008581644815256258\n",
            "Common Lisp :\t 0.007151370679380214\n",
            "formed a coalition with :\t 0.0057210965435041715\n",
            "orbit each other every :\t 0.0057210965435041715\n",
            "giant star of magnitude :\t 0.0057210965435041715\n",
            "Belfast Giants :\t 0.004290822407628129\n",
            "deposits of graphite :\t 0.004290822407628129\n",
            "formed a coalition :\t 0.004290822407628129\n",
            "Odyssey Arena :\t 0.0028605482717520858\n",
            "big bang :\t 0.0028605482717520858\n",
            "spoken natively :\t 0.0028605482717520858\n",
            "life expectancy :\t 0.0028605482717520858\n",
            "casuistic reasoning :\t 0.0028605482717520858\n",
            "real humans :\t 0.0028605482717520858\n",
            "Liberty Cup :\t 0.0028605482717520858\n",
            "\n",
            "\n",
            "Doc 4\n",
            "--------------------------------------------------\n",
            "chimp will :\t 0.03252032520325203\n",
            "from dub :\t 0.03252032520325203\n",
            "richest province :\t 0.03252032520325203\n",
            "facial expressions :\t 0.029257127418276626\n",
            "\n",
            "\n",
            "Doc 5\n",
            "--------------------------------------------------\n",
            "hop music and culture :\t 0.007897334649555774\n",
            "used as a unit :\t 0.007897334649555774\n",
            "rainfall amounts and intensities :\t 0.007897334649555774\n",
            "approximately the same :\t 0.00592300098716683\n",
            "Dar es Salaam :\t 0.00592300098716683\n",
            "music and culture :\t 0.00592300098716683\n",
            "floating point operations :\t 0.00592300098716683\n",
            "Canary Wharf :\t 0.00592300098716683\n",
            "encyclopedia articles :\t 0.00592300098716683\n",
            "amounts and intensities :\t 0.00592300098716683\n",
            "fees and fines :\t 0.00592300098716683\n",
            "vacuum tube :\t 0.005328667333338635\n",
            "had already been :\t 0.004254495478747765\n",
            "hop music :\t 0.003948667324777887\n",
            "Tel Dan :\t 0.003948667324777887\n",
            "\n",
            "\n",
            "Doc 6\n",
            "--------------------------------------------------\n",
            "formal languages :\t 0.024057738572574178\n",
            "formal languages are used :\t 0.019246190858059342\n",
            "no family names :\t 0.014434643143544507\n",
            "Newton's First Law :\t 0.014434643143544507\n",
            "constant velocity :\t 0.014434643143544507\n",
            "net force :\t 0.014434643143544507\n",
            "languages are used :\t 0.014434643143544507\n",
            "flag should :\t 0.009623095429029671\n",
            "which is now :\t 0.009242111085576784\n",
            "Ne Win :\t 0.008657481970604711\n",
            "\n",
            "\n",
            "Doc 7\n",
            "--------------------------------------------------\n",
            "of the Foreign Legion :\t 0.2857142857142857\n",
            "Foreign Legion :\t 0.2857142857142857\n",
            "of the Foreign :\t 0.19278357173828703\n",
            "\n",
            "\n",
            "Doc 8\n",
            "--------------------------------------------------\n",
            "kinetic energy of the mass :\t 0.011679211004412146\n",
            "speaking it is the area west :\t 0.009343368803529718\n",
            "energy of the mass :\t 0.009343368803529718\n",
            "proven reserves in the world :\t 0.007786140669608097\n",
            "speaking it is the area :\t 0.007786140669608097\n",
            "real algebraic numbers :\t 0.007007526602647288\n",
            "sequence of real :\t 0.007007526602647288\n",
            "Parliament of England :\t 0.007007526602647288\n",
            "real numbers :\t 0.007004853372271007\n",
            "reserves in the world :\t 0.0062289125356864775\n",
            "not in the sequence :\t 0.0062289125356864775\n",
            "largest in Africa :\t 0.004671684401764859\n",
            "Government of Ghana :\t 0.004671684401764859\n",
            "Lynne and Petty :\t 0.004671684401764859\n",
            "Walk of Fame :\t 0.004671684401764859\n",
            "\n",
            "\n",
            "Doc 9\n",
            "--------------------------------------------------\n",
            "steppe nomads and peoples :\t 0.007799805004874878\n",
            "Grand Master :\t 0.007799805004874878\n",
            "named in his honour :\t 0.007017146582609084\n",
            "intercalary years :\t 0.005849853753656159\n",
            "not that it's :\t 0.005849853753656158\n",
            "nomads and peoples :\t 0.005849853753656158\n",
            "may not cross :\t 0.005849853753656158\n",
            "Kent State :\t 0.005262859936956812\n",
            "Mexican history :\t 0.003899902502437439\n",
            "Anthony Quinn :\t 0.003899902502437439\n",
            "extended format :\t 0.003899902502437439\n",
            "Holyland Tower :\t 0.003899902502437439\n",
            "steppe nomads :\t 0.003899902502437439\n",
            "Asian Avars :\t 0.003899902502437439\n",
            "dog meat :\t 0.003899902502437439\n",
            "\n",
            "\n",
            "Doc 10\n",
            "--------------------------------------------------\n",
            "linear maps corresponds to the matrix :\t 0.020224719101123594\n",
            "maps corresponds to the matrix :\t 0.016853932584269662\n",
            "defective pixels :\t 0.01348314606741573\n",
            "corresponds to the matrix :\t 0.01348314606741573\n",
            "LCD panel :\t 0.010112359550561799\n",
            "linear maps :\t 0.010112359550561799\n",
            "what would become :\t 0.008504085658248327\n",
            "my mother :\t 0.006741573033707865\n",
            "trading estate :\t 0.006741573033707865\n",
            "SVGA LCD :\t 0.006741573033707865\n",
            "word pairs :\t 0.006741573033707865\n",
            "Lisp programmers :\t 0.006741573033707865\n",
            "situation could :\t 0.006065101133339369\n",
            "at home :\t 0.004316446638095225\n",
            "such as :\t 0.003097155506151237\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RLSBBaV5t8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_doc = dict()\n",
        "\n",
        "for i in range(len(REs)):\n",
        "    for w in list(REs[i].keys()):\n",
        "        try:\n",
        "            word_doc[w].add(i)\n",
        "        except KeyError:\n",
        "            word_doc[w] = set()\n",
        "            word_doc[w].add(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVM7g59m74Cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_keywords = dict()\n",
        "\n",
        "for w1 in word_doc:\n",
        "    for w2 in word_doc:\n",
        "        if w1 != w2:\n",
        "            try:\n",
        "                im_keywords[w1].append([w2, len(set.intersection(word_doc[w1], word_doc[w2]))])\n",
        "            except KeyError:\n",
        "                im_keywords[w1] = []\n",
        "                im_keywords[w1].append([w2, len(set.intersection(word_doc[w1], word_doc[w2]))])\n",
        "            \n",
        "            try:\n",
        "                im_keywords[w2].append([w1, len(set.intersection(word_doc[w1], word_doc[w2]))])\n",
        "            except KeyError:\n",
        "                im_keywords[w2] = []\n",
        "                im_keywords[w2].append([w1, len(set.intersection(word_doc[w1], word_doc[w2]))])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-bnrVBZBkI0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17437
        },
        "outputId": "ec77f1f9-258f-4006-9f8b-62f6658952a5"
      },
      "source": [
        "print(\"--------------------------------------------------\")\n",
        "print(\"TOP Implicit Keywords for document 1\")\n",
        "print(\"--------------------------------------------------\\n\")\n",
        "\n",
        "top0 = sorted(ex_keywords[0].items(), key=lambda kv: -kv[1])[:15]\n",
        "\n",
        "# for k, v in top0:\n",
        "#     print(k, v)\n",
        "#     #print(im_keywords[k])\n",
        "    \n",
        "im_keywords[top0[0][0]]\n",
        "\n",
        "# for i, l in enumerate(ex_keywords[:10]):\n",
        "#     print(\"Doc\", i+1)\n",
        "#     print(\"--------------------------------------------------\")\n",
        "\n",
        "#     for elem, val in :\n",
        "#         print(elem, \":\\t\", val)\n",
        "#     print('\\n')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "TOP Implicit Keywords for document 1\n",
            "--------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['in the', 1],\n",
              " ['of the', 1],\n",
              " ['John Philoponus', 1],\n",
              " ['body parts', 1],\n",
              " ['the end of', 1],\n",
              " ['civil war', 1],\n",
              " ['as well as', 1],\n",
              " ['Indic scripts', 1],\n",
              " ['Southeast Asia', 1],\n",
              " ['attached to', 1],\n",
              " ['a monastery', 1],\n",
              " ['Orthodox Church', 1],\n",
              " ['elevated to', 1],\n",
              " ['have been', 1],\n",
              " ['may be given', 1],\n",
              " ['in the', 1],\n",
              " ['of the', 1],\n",
              " ['John Philoponus', 1],\n",
              " ['body parts', 1],\n",
              " ['the end of', 1],\n",
              " ['civil war', 1],\n",
              " ['as well as', 1],\n",
              " ['Indic scripts', 1],\n",
              " ['Southeast Asia', 1],\n",
              " ['attached to', 1],\n",
              " ['a monastery', 1],\n",
              " ['Orthodox Church', 1],\n",
              " ['elevated to', 1],\n",
              " ['have been', 1],\n",
              " ['may be given', 1],\n",
              " ['given to any', 1],\n",
              " ['serve as', 1],\n",
              " ['head of a monastery', 1],\n",
              " ['World War', 1],\n",
              " ['World War II', 1],\n",
              " ['UEFA Cup', 1],\n",
              " ['United States', 1],\n",
              " ['His work', 1],\n",
              " ['the history of', 1],\n",
              " ['murder of Naboth', 1],\n",
              " ['adverse events', 1],\n",
              " ['He was', 1],\n",
              " ['his life', 1],\n",
              " ['New Orleans', 1],\n",
              " ['Pope Honorius', 1],\n",
              " ['depart for', 1],\n",
              " ['depart for the Holy', 1],\n",
              " ['depart for the Holy Land', 1],\n",
              " ['Holy Land', 1],\n",
              " ['a letter', 1],\n",
              " ['Latin Empire', 1],\n",
              " ['Peter of Courtenay', 1],\n",
              " ['whom he had', 1],\n",
              " ['second and third', 1],\n",
              " ['crime rate', 1],\n",
              " ['below the national', 1],\n",
              " ['below the national average', 1],\n",
              " ['national average', 1],\n",
              " ['was given', 1],\n",
              " ['Louis the Child', 1],\n",
              " ['population pyramid', 1],\n",
              " ['maximum transfer', 1],\n",
              " ['megabytes per', 1],\n",
              " ['removal of a hydrogen', 1],\n",
              " ['removal of a hydrogen ion', 1],\n",
              " ['hydrogen ion', 1],\n",
              " ['Lee Hooker', 1],\n",
              " ['Bosnia and Herzegovina', 1],\n",
              " ['volleyball at', 1],\n",
              " ['always respect', 1],\n",
              " ['tuning pegs', 1],\n",
              " ['form of English', 1],\n",
              " ['small enough', 1],\n",
              " ['enough to fit', 1],\n",
              " ['football clubs', 0],\n",
              " ['Premier League', 0],\n",
              " ['struck in', 0],\n",
              " ['by a cannonball', 0],\n",
              " ['other countries', 0],\n",
              " ['an independent', 0],\n",
              " ['an independent record', 0],\n",
              " ['black metal', 0],\n",
              " ['Belfast Giants', 0],\n",
              " ['Odyssey Arena', 0],\n",
              " ['common law', 0],\n",
              " ['For example', 0],\n",
              " ['deposits of graphite', 0],\n",
              " ['North Korea', 0],\n",
              " ['Czech Republic', 0],\n",
              " ['enough time', 0],\n",
              " ['big bang', 0],\n",
              " ['spoken natively', 0],\n",
              " ['life expectancy', 0],\n",
              " ['scale length', 0],\n",
              " ['such as', 0],\n",
              " ['casuistic reasoning', 0],\n",
              " ['Labour government', 0],\n",
              " ['formed a coalition', 0],\n",
              " ['formed a coalition with', 0],\n",
              " ['Common Lisp', 0],\n",
              " ['the movie', 0],\n",
              " ['real humans', 0],\n",
              " ['Liberty Cup', 0],\n",
              " ['Coronae Borealis', 0],\n",
              " ['orbit each', 0],\n",
              " ['orbit each other every', 0],\n",
              " ['The primary', 0],\n",
              " ['giant star', 0],\n",
              " ['giant star of magnitude', 0],\n",
              " ['escapement mechanism', 0],\n",
              " ['at least', 0],\n",
              " ['which terminates', 0],\n",
              " [\"Vicar's Oak\", 0],\n",
              " ['show surprising', 0],\n",
              " ['lower classes', 0],\n",
              " ['facial expressions', 0],\n",
              " ['chimp will', 0],\n",
              " ['from dub', 0],\n",
              " ['richest province', 0],\n",
              " ['vacuum tube', 0],\n",
              " ['approximately the same', 0],\n",
              " ['included in', 0],\n",
              " ['Dar es Salaam', 0],\n",
              " ['hip hop', 0],\n",
              " ['hop music', 0],\n",
              " ['hop music and culture', 0],\n",
              " ['music and culture', 0],\n",
              " ['Tel Dan', 0],\n",
              " ['Mesha Stele', 0],\n",
              " ['floating point operations', 0],\n",
              " ['per second', 0],\n",
              " ['double bass', 0],\n",
              " ['Canary Wharf', 0],\n",
              " ['there is no', 0],\n",
              " ['had already been', 0],\n",
              " ['On January', 0],\n",
              " ['New York', 0],\n",
              " ['Alpha processors', 0],\n",
              " ['two digits', 0],\n",
              " ['end of', 0],\n",
              " ['used as a unit', 0],\n",
              " ['encyclopedia articles', 0],\n",
              " ['dictionary entries', 0],\n",
              " ['European Council', 0],\n",
              " ['rainfall amounts', 0],\n",
              " ['rainfall amounts and intensities', 0],\n",
              " ['amounts and intensities', 0],\n",
              " ['biomass production', 0],\n",
              " ['fees and fines', 0],\n",
              " ['Historical Society', 0],\n",
              " ['ENSO may', 0],\n",
              " ['civil conflicts', 0],\n",
              " ['which is now', 0],\n",
              " ['no family names', 0],\n",
              " ['Ne Win', 0],\n",
              " [\"Newton's First Law\", 0],\n",
              " ['constant velocity', 0],\n",
              " ['net force', 0],\n",
              " ['formal languages', 0],\n",
              " ['formal languages are used', 0],\n",
              " ['languages are used', 0],\n",
              " ['flag should', 0],\n",
              " ['of the Foreign', 0],\n",
              " ['of the Foreign Legion', 0],\n",
              " ['Foreign Legion', 0],\n",
              " ['wide range', 0],\n",
              " ['Emperor Menelik', 0],\n",
              " ['points out', 0],\n",
              " ['crude oil', 0],\n",
              " ['largest in Africa', 0],\n",
              " ['proven reserves', 0],\n",
              " ['proven reserves in the world', 0],\n",
              " ['reserves in the world', 0],\n",
              " ['natural gas', 0],\n",
              " ['Government of Ghana', 0],\n",
              " ['real algebraic numbers', 0],\n",
              " ['sequence of real', 0],\n",
              " ['real numbers', 0],\n",
              " ['not in the sequence', 0],\n",
              " ['can be', 0],\n",
              " ['transcendental numbers', 0],\n",
              " ['Euler integral', 0],\n",
              " ['Lynne and Petty', 0],\n",
              " ['Walk of Fame', 0],\n",
              " ['front of', 0],\n",
              " ['reduction in tariffs', 0],\n",
              " ['The neighborhood', 0],\n",
              " ['Hudson River', 0],\n",
              " ['East Village', 0],\n",
              " ['Greenwich Village', 0],\n",
              " ['dividing line', 0],\n",
              " ['speaking it', 0],\n",
              " ['speaking it is', 0],\n",
              " ['speaking it is the area', 0],\n",
              " ['speaking it is the area west', 0],\n",
              " ['area west', 0],\n",
              " [\"New York's\", 0],\n",
              " ['photo of', 0],\n",
              " ['its own', 0],\n",
              " ['progressive rock', 0],\n",
              " ['objects on', 0],\n",
              " ['layout engine', 0],\n",
              " ['Parliament of England', 0],\n",
              " ['Working Group', 0],\n",
              " ['National Transitional', 0],\n",
              " ['National Transitional Council', 0],\n",
              " ['dress up', 0],\n",
              " ['fun event', 0],\n",
              " ['When the spring', 0],\n",
              " ['kinetic energy', 0],\n",
              " ['kinetic energy of the mass', 0],\n",
              " ['energy of the mass', 0],\n",
              " ['potential energy', 0],\n",
              " ['matches won', 0],\n",
              " ['scholars such', 0],\n",
              " ['his plays', 0],\n",
              " ['Mexican history', 0],\n",
              " ['named in his honour', 0],\n",
              " ['his honour', 0],\n",
              " ['Anthony Quinn', 0],\n",
              " ['Mexico City', 0],\n",
              " ['has also been', 0],\n",
              " ['the inner', 0],\n",
              " ['In response', 0],\n",
              " ['extended format', 0],\n",
              " ['England and Wales', 0],\n",
              " ['would have', 0],\n",
              " [\"not that it's\", 0],\n",
              " ['South Korea', 0],\n",
              " ['two countries', 0],\n",
              " ['intercalary years', 0],\n",
              " ['Holyland Tower', 0],\n",
              " ['Roman Empire', 0],\n",
              " ['steppe nomads', 0],\n",
              " ['steppe nomads and peoples', 0],\n",
              " ['nomads and peoples', 0],\n",
              " ['Asian Avars', 0],\n",
              " ['as early as', 0],\n",
              " ['dog meat', 0],\n",
              " ['Grand Master', 0],\n",
              " ['may not cross', 0],\n",
              " ['Kent State', 0],\n",
              " ['According to', 0],\n",
              " ['situation could', 0],\n",
              " ['known as', 0],\n",
              " ['at home', 0],\n",
              " ['what would become', 0],\n",
              " ['my mother', 0],\n",
              " ['the country', 0],\n",
              " ['trading estate', 0],\n",
              " ['SVGA LCD', 0],\n",
              " ['LCD panel', 0],\n",
              " ['defective pixels', 0],\n",
              " ['word pairs', 0],\n",
              " ['Lisp programmers', 0],\n",
              " ['linear maps', 0],\n",
              " ['linear maps corresponds to the matrix', 0],\n",
              " ['maps corresponds to the matrix', 0],\n",
              " ['corresponds to the matrix', 0],\n",
              " ['two vectors', 0],\n",
              " [\"against Labor's\", 0],\n",
              " ['Coalition was returned', 0],\n",
              " ['returned again', 0],\n",
              " ['Labor Party', 0],\n",
              " ['thine is the kingdom', 0],\n",
              " ['part of', 0],\n",
              " ['descriptive markup', 0],\n",
              " ['one metre', 0],\n",
              " ['National Assembly', 0],\n",
              " ['Drag Racing', 0],\n",
              " ['the same', 0],\n",
              " ['real estate', 0],\n",
              " ['boom in', 0],\n",
              " ['The rise', 0],\n",
              " ['spread out across', 0],\n",
              " ['neighboring Illinois', 0],\n",
              " ['over the winner', 0],\n",
              " ['Barack Obama', 0],\n",
              " ['fermented milk', 0],\n",
              " ['MS NL', 0],\n",
              " ['Mannerist style', 0],\n",
              " ['other areas', 0],\n",
              " [\"the world's\", 0],\n",
              " ['always write', 0],\n",
              " ['arduous task', 0],\n",
              " ['Men at Work', 0],\n",
              " ['Live Aid', 0],\n",
              " ['set up', 0],\n",
              " ['Alexander II', 0],\n",
              " ['musical theatre', 0],\n",
              " ['theatre in Europe', 0],\n",
              " ['would move', 0],\n",
              " ['part of the story', 0],\n",
              " ['and the next', 0],\n",
              " ['network segment', 0],\n",
              " ['his marriage to Catherine', 0],\n",
              " ['marriage to Catherine', 0],\n",
              " ['based on', 0],\n",
              " ['King Nikola', 0],\n",
              " ['King Nikola I', 0],\n",
              " ['inner journey', 0],\n",
              " ['LG ICC', 0],\n",
              " ['foreign minister', 0],\n",
              " ['percent of that figure', 0],\n",
              " ['that figure', 0],\n",
              " ['colonial revolt', 0],\n",
              " ['Nova Scotia', 0],\n",
              " ['Academy Award nominee', 0],\n",
              " ['Northern Ireland', 0],\n",
              " ['Territory electorate', 0],\n",
              " ['polynomial time', 0],\n",
              " ['the language', 0],\n",
              " ['beam amplitude', 0],\n",
              " ['new stadium', 0],\n",
              " ['had previously', 0],\n",
              " ['weights and measures', 0],\n",
              " ['national physical', 0],\n",
              " ['Inspector General', 0],\n",
              " ['head of', 0],\n",
              " ['head of the Office', 0],\n",
              " ['General Counsel', 0],\n",
              " ['Director of Compliance', 0],\n",
              " ['suggested that', 0],\n",
              " ['N rays', 0],\n",
              " ['observers have', 0],\n",
              " ['nuclear weapons', 0],\n",
              " ['can be used', 0],\n",
              " ['hand side', 0],\n",
              " ['green alternates', 0],\n",
              " ['alternate jersey', 0],\n",
              " ['white with gold', 0],\n",
              " ['Soviet Union', 0],\n",
              " ['Soviet Union and Germany', 0],\n",
              " ['Union and Germany', 0],\n",
              " ['that country', 0],\n",
              " ['may have', 0],\n",
              " ['ibn Ali', 0],\n",
              " ['law and order', 0],\n",
              " ['means of communication', 0],\n",
              " ['almost all', 0],\n",
              " ['maintaining law', 0],\n",
              " ['maintaining law and order', 0],\n",
              " ['early days', 0],\n",
              " ['court of last', 0],\n",
              " ['life on Earth', 0],\n",
              " ['two sounds', 0],\n",
              " ['two sounds are perceived', 0],\n",
              " ['sounds are perceived', 0],\n",
              " ['ends of the world', 0],\n",
              " ['Arctic Circle', 0],\n",
              " ['armed forces', 0],\n",
              " ['eighth book', 0],\n",
              " ['wide variety', 0],\n",
              " ['DNA templates', 0],\n",
              " ['designed using', 0],\n",
              " ['Benedict IX', 0],\n",
              " ['Sylvester III', 0],\n",
              " ['Count of Flanders', 0],\n",
              " ['average energy', 0],\n",
              " ['military budget', 0],\n",
              " ['trillion rubles', 0],\n",
              " ['than any', 0],\n",
              " ['lay in repose', 0],\n",
              " ['West Virginia', 0],\n",
              " ['his blog', 0],\n",
              " ['red coats', 0],\n",
              " ['coats dyed', 0],\n",
              " ['coats dyed with', 0],\n",
              " ['to become', 0],\n",
              " ['Rajiv Gandhi', 0],\n",
              " ['President Zail Singh', 0],\n",
              " ['Congress party', 0],\n",
              " ['Six Nations', 0],\n",
              " ['home matches', 0],\n",
              " ['Stadio Olimpico', 0],\n",
              " ['city of Rome', 0],\n",
              " ['closest to', 0],\n",
              " ['dealt face', 0],\n",
              " ['Players are not', 0],\n",
              " ['Players are not allowed', 0],\n",
              " ['Players are not allowed to look', 0],\n",
              " ['not allowed', 0],\n",
              " ['not allowed to look', 0],\n",
              " ['allowed to look', 0],\n",
              " ['playing one', 0],\n",
              " ['playing one of his', 0],\n",
              " ['one of his', 0],\n",
              " ['the Asian', 0],\n",
              " ['referred to', 0],\n",
              " ['Air Force', 0],\n",
              " ['Sicilian cuisine', 0],\n",
              " ['As well', 0],\n",
              " ['meat dishes', 0],\n",
              " ['value can', 0],\n",
              " ['Akal Takht', 0],\n",
              " ['more general', 0],\n",
              " ['New York Stock Exchange', 0],\n",
              " ['features of', 0],\n",
              " ['steam engine', 0],\n",
              " ['board sends', 0],\n",
              " ['Saint John', 0],\n",
              " ['melting range', 0],\n",
              " ['the AFL', 0],\n",
              " ['noted that', 0],\n",
              " ['halftime show', 0],\n",
              " ['Seattle University', 0],\n",
              " ['basketball team', 0],\n",
              " ['radial speed', 0],\n",
              " ['Point Mugu', 0],\n",
              " ['placed on', 0],\n",
              " ['exchange agreements', 0],\n",
              " ['who was awarded', 0],\n",
              " ['only those', 0],\n",
              " ['those results', 0],\n",
              " ['people of Israel', 0],\n",
              " ['trigonometric functions', 0],\n",
              " ['Turing machine', 0],\n",
              " ['Ramada Plaza', 0],\n",
              " ['hotel became', 0],\n",
              " ['one of the most', 0],\n",
              " ['one of the most important', 0],\n",
              " ['chief negotiator', 0],\n",
              " ['Tipu Sultan', 0],\n",
              " [\"Tipu Sultan's\", 0],\n",
              " ['Tarot de Marseille', 0],\n",
              " ['cards based', 0],\n",
              " ['Marseille design', 0],\n",
              " ['surface at', 0],\n",
              " ['one of', 0],\n",
              " ['has been', 0],\n",
              " ['Republic of China', 0],\n",
              " ['deliver the commissions', 0],\n",
              " ['former slaves', 0],\n",
              " ['Thirteenth Amendment', 0],\n",
              " ['congressional representation', 0],\n",
              " ['working class', 0],\n",
              " ['first time', 0],\n",
              " ['living organisms', 0],\n",
              " ['organic chemistry', 0],\n",
              " [\"Coast Guard's\", 0],\n",
              " ['legal authority', 0],\n",
              " ['Coast Guard', 0],\n",
              " ['Venezuelans live', 0],\n",
              " ['south of', 0],\n",
              " ['south of the Orinoco', 0],\n",
              " ['that it is', 0],\n",
              " ['known as the Year', 0],\n",
              " ['following day', 0],\n",
              " ['Viet Minh', 0],\n",
              " ['French Union', 0],\n",
              " ['rapidly changing', 0],\n",
              " ['changing pages', 0],\n",
              " ['less frequently', 0],\n",
              " ['Cypress Hill', 0],\n",
              " ['Muggs produced', 0],\n",
              " ['GZA appeared', 0],\n",
              " ['construction of', 0],\n",
              " ['West Berlin', 0],\n",
              " ['the frequency of', 0],\n",
              " ['higher energy', 0],\n",
              " ['Olympic Games', 0],\n",
              " [\"'one hand'\", 0],\n",
              " [\"'two hands'\", 0],\n",
              " ['winner of', 0],\n",
              " ['gray wolf', 0],\n",
              " ['Gray wolf weight', 0],\n",
              " ['heaviest recorded', 0],\n",
              " ['York is', 0],\n",
              " ['under the Local', 0],\n",
              " ['under the Local Government', 0],\n",
              " ['under the Local Government Act', 0],\n",
              " ['Local Government', 0],\n",
              " ['systems for conversions', 0],\n",
              " ['common year starting', 0],\n",
              " ['common year starting on Friday', 0],\n",
              " ['starting on Friday', 0],\n",
              " ['link will display', 0],\n",
              " ['link will display the full', 0],\n",
              " ['link will display the full calendar', 0],\n",
              " ['display the full', 0],\n",
              " ['display the full calendar', 0],\n",
              " ['full calendar', 0],\n",
              " ['Julian calendar', 0],\n",
              " ['end of World', 0],\n",
              " ['end of World War I', 0],\n",
              " ['Eighth Crusade', 0],\n",
              " ['grade was double', 0],\n",
              " ['grade was double tracked', 0],\n",
              " ['double tracked', 0],\n",
              " ['twentieth century', 0],\n",
              " ['whole grain', 0],\n",
              " ['rice phytoliths', 0],\n",
              " ['wild rice', 0],\n",
              " ['IWW organized', 0],\n",
              " ['friction levels', 0],\n",
              " ['hard seed coats', 0],\n",
              " ['seed coat', 0],\n",
              " ['Norman French', 0],\n",
              " ['Old English', 0],\n",
              " ['early Middle English', 0],\n",
              " ['being spoken', 0],\n",
              " ['Life of St', 0],\n",
              " ['last generation', 0],\n",
              " ['first measurement', 0],\n",
              " ['for the second', 0],\n",
              " ['than the Sun', 0],\n",
              " ['One of', 0],\n",
              " ['wild mammals', 0],\n",
              " ['for every', 0],\n",
              " ['same shape', 0],\n",
              " ['more than', 0],\n",
              " ['Formula One', 0],\n",
              " ['radio waves', 0],\n",
              " ['finite impulse', 0],\n",
              " ['finite impulse response filters', 0],\n",
              " ['a number of', 0],\n",
              " ['reflection coefficient', 0],\n",
              " ['driven elements', 0],\n",
              " ['metal sheet', 0],\n",
              " ['Merchandise Mart', 0],\n",
              " ['the late', 0],\n",
              " ['The resulting', 0],\n",
              " ['Chinese defeated', 0],\n",
              " ['Dukes and Hogg', 0],\n",
              " ['Lie algebra', 0],\n",
              " ['special linear', 0],\n",
              " ['special linear group', 0],\n",
              " ['linear group', 0],\n",
              " ['do not change', 0],\n",
              " ['do not change volume', 0],\n",
              " ['change volume', 0],\n",
              " ['heated to a boil', 0],\n",
              " ['receiving flask', 0],\n",
              " ['Cambridge Circus', 0],\n",
              " ['Charing Cross', 0],\n",
              " ['Government of National Defence', 0],\n",
              " ['National Defence', 0],\n",
              " ['extrasolar planets', 0],\n",
              " ['brown dwarfs', 0],\n",
              " ['class society', 0],\n",
              " ['Economic Activities', 0],\n",
              " ['sugar confectionery', 0],\n",
              " ['confectionery manufacturing', 0],\n",
              " ['National Park', 0],\n",
              " ['argued that the region', 0],\n",
              " ['argued that the region should', 0],\n",
              " ['region should', 0],\n",
              " ['national park', 0],\n",
              " ['Lake Chelan', 0],\n",
              " ['Forest Service', 0],\n",
              " ['set aside', 0],\n",
              " ['North Cascades', 0],\n",
              " ['North Cascades National Park Act', 0],\n",
              " ['Cascades National Park Act', 0],\n",
              " ['bus station', 0],\n",
              " ['Chairman Mao', 0],\n",
              " ['we must continue', 0],\n",
              " ['Times Square', 0],\n",
              " ['a natural', 0],\n",
              " ['assured destruction', 0],\n",
              " ['each prime', 0],\n",
              " ['first major', 0],\n",
              " ['mobile phones', 0],\n",
              " ['southern part', 0],\n",
              " ['southern part of the State', 0],\n",
              " ['part of the State', 0],\n",
              " ['who wrote', 0],\n",
              " ['to the', 0],\n",
              " ['average temperature', 0],\n",
              " ['physical determinism', 0],\n",
              " ['theological determinism', 0],\n",
              " ['respect to theological', 0],\n",
              " ['respect to theological determinism', 0],\n",
              " ['might be classified', 0],\n",
              " ['hard incompatibilism', 0],\n",
              " ['incompatibilism with respect', 0],\n",
              " ['incompatibilism with respect to physical', 0],\n",
              " ['respect to physical', 0],\n",
              " ['respect to physical determinism', 0],\n",
              " ['metaphysical libertarianism', 0],\n",
              " ['if it was assumed', 0],\n",
              " ['login name', 0],\n",
              " ['This includes', 0],\n",
              " ['high temperature', 0],\n",
              " ['Holy Roman Emperor', 0],\n",
              " ['Maria Theresa', 0],\n",
              " ['elder brother', 0],\n",
              " ['Wilhelmine Amalia', 0],\n",
              " ['other parts', 0],\n",
              " ['side of the square', 0],\n",
              " ['fourth side', 0],\n",
              " ['an increase', 0],\n",
              " ['net increase', 0],\n",
              " ['dark environments', 0],\n",
              " ['molecular biology', 0],\n",
              " ['only a single', 0],\n",
              " ['a series of', 0],\n",
              " ['before he died', 0],\n",
              " ['it can be made', 0],\n",
              " ['can be made', 0],\n",
              " ['at this time', 0],\n",
              " ['Second Sophistic', 0],\n",
              " ['Takoma Park', 0],\n",
              " ['bass cornett', 0],\n",
              " ['also used', 0],\n",
              " ['entrance examination', 0],\n",
              " ['reflected binary code', 0],\n",
              " ['idea of', 0],\n",
              " ['Post Office', 0],\n",
              " ['lower reaches', 0],\n",
              " ['drain into', 0],\n",
              " ['Middle East', 0],\n",
              " ['what is now', 0],\n",
              " ['raw data', 0],\n",
              " ['Radon transformation', 0],\n",
              " ['during the Napoleonic', 0],\n",
              " ['during the Napoleonic Wars', 0],\n",
              " ['Napoleonic Wars', 0],\n",
              " ['inhibit angiogenesis', 0],\n",
              " ['Indonesian words', 0],\n",
              " ['ZFC with', 0],\n",
              " ['continuum hypothesis', 0],\n",
              " ['continuum hypothesis we', 0],\n",
              " ['continuum hypothesis we can prove', 0],\n",
              " ['hypothesis we can prove', 0],\n",
              " ['we can prove', 0],\n",
              " ['running EIGRP', 0],\n",
              " ['exchanged between', 0],\n",
              " ['Aryan Nations', 0],\n",
              " ['Warner Bros', 0],\n",
              " ['Daffy Duck', 0],\n",
              " ['Daffy has also', 0],\n",
              " ['globular proteins', 0],\n",
              " ['His remains', 0],\n",
              " ['Conservative candidate', 0],\n",
              " ['first order conditions', 0],\n",
              " ['eye protection', 0],\n",
              " ['at the time', 0],\n",
              " ['Tom Swift', 0],\n",
              " ['Hardy Boys', 0],\n",
              " [\"People's Committees\", 0],\n",
              " ['next door', 0],\n",
              " ['Burton Latimer', 0],\n",
              " ['responsible for', 0],\n",
              " ['Army Air', 0],\n",
              " ['along with', 0],\n",
              " ['his wife', 0],\n",
              " ['boxed set', 0],\n",
              " ['Spell Law', 0],\n",
              " ['risk of stroke', 0],\n",
              " [\"Machault d'Arnouville\", 0],\n",
              " ['measurement problem', 0],\n",
              " ['These authors', 0],\n",
              " ['Han dynasty', 0],\n",
              " ['Northern Xiongnu', 0],\n",
              " [\"brothers' inclusion\", 0],\n",
              " ['Inner Mongolia', 0],\n",
              " ['Silver Purchase', 0],\n",
              " ['Senator John Sherman', 0],\n",
              " ['conference committee', 0],\n",
              " ['agreement on', 0],\n",
              " ['municipal autonomy', 0],\n",
              " ['Austrian Empire', 0],\n",
              " ['private company', 0],\n",
              " ['tuna fish', 0],\n",
              " ['Harford County', 0],\n",
              " ['The current', 0],\n",
              " ['Canyon National', 0],\n",
              " ['about north', 0],\n",
              " ['National Monument', 0],\n",
              " ['the Middle', 0],\n",
              " ['incorporated as', 0],\n",
              " ['with a population', 0],\n",
              " ['the second', 0],\n",
              " ['Orient Express', 0],\n",
              " ['her husband', 0],\n",
              " ['health care', 0],\n",
              " ['be supplied', 0],\n",
              " ['dynamic information', 0],\n",
              " ['uniquely identify', 0],\n",
              " ['pornographic movie', 0],\n",
              " [\"Osiris' corpse\", 0],\n",
              " ['days later', 0],\n",
              " ['Jesus Christ', 0],\n",
              " ['Jesus Christ has two', 0],\n",
              " ['Christ has two', 0],\n",
              " ['two natures', 0],\n",
              " ['accumulation point', 0],\n",
              " ['an open', 0],\n",
              " ['MTV Video Music', 0],\n",
              " ['my tears', 0],\n",
              " ['Music Awards', 0],\n",
              " ['the original', 0],\n",
              " ['attack plan', 0],\n",
              " ['Lunga perimeter', 0],\n",
              " ['Carpiquet Airfield', 0],\n",
              " ['solo acoustic', 0],\n",
              " ['demo tape', 0],\n",
              " ['closely related', 0],\n",
              " ['direction of its', 0],\n",
              " ['its spin', 0],\n",
              " ['relative to its', 0],\n",
              " ['massive particles', 0],\n",
              " ['Opa proteins', 0],\n",
              " ['immune cells', 0],\n",
              " ['was the Minister', 0],\n",
              " ['run in', 0],\n",
              " ['hundred thousand', 0],\n",
              " [\"Laverack's best\", 0],\n",
              " ['Ceuta and Melilla', 0],\n",
              " ['Morris Air', 0],\n",
              " ['competing airline', 0],\n",
              " ['before eating', 0],\n",
              " ['it did', 0],\n",
              " ['Schiller wrote', 0],\n",
              " [\"Thirty Years'\", 0],\n",
              " [\"Thirty Years' War\", 0],\n",
              " ['National Register', 0],\n",
              " ['indigenous Maya', 0],\n",
              " ['Valley Railroad', 0],\n",
              " ['Delaware and Hudson', 0],\n",
              " ['correct pronunciation', 0],\n",
              " ['South Park', 0],\n",
              " ['West Park', 0],\n",
              " ['industrial park', 0],\n",
              " ['major participant', 0],\n",
              " ['heart attack', 0],\n",
              " ['no longer', 0],\n",
              " ['POWs and Disarmed', 0],\n",
              " ['POWs and Disarmed Enemy Forces', 0],\n",
              " ['Disarmed Enemy Forces', 0],\n",
              " ['In the case', 0],\n",
              " ['In the case of an', 0],\n",
              " ['In the case of an eclipse', 0],\n",
              " ['case of an', 0],\n",
              " ['case of an eclipse', 0],\n",
              " ['an eclipse', 0],\n",
              " ['means that', 0],\n",
              " ['visible from the same', 0],\n",
              " ['that the Republic', 0],\n",
              " ['that the Republic of China', 0],\n",
              " ['stacked cars', 0],\n",
              " ['Kama Sutra', 0],\n",
              " ['popularly known', 0],\n",
              " ['Thousand Nights', 0],\n",
              " ['maternal blood', 0],\n",
              " ['the Austrian', 0],\n",
              " ['for her role in', 0],\n",
              " ['her role', 0],\n",
              " ['she received', 0],\n",
              " ['Berlin International Film Festival', 0],\n",
              " ['Volga Germans', 0],\n",
              " ['well known', 0],\n",
              " ['La Union', 0],\n",
              " [\"league's top\", 0],\n",
              " ['National Gallery', 0],\n",
              " ['Art Gallery', 0],\n",
              " ['New Zealand', 0],\n",
              " ['she was born', 0],\n",
              " ['recorded and released', 0],\n",
              " ['Canadian Corps', 0],\n",
              " ['tactical analysis', 0],\n",
              " ['the new', 0],\n",
              " ['over all', 0],\n",
              " ['Codex Regius', 0],\n",
              " ['Hawaii County', 0],\n",
              " ['authority is vested', 0],\n",
              " ['lymph sacs', 0],\n",
              " ['posterior lymph sacs', 0],\n",
              " ['civil unions', 0],\n",
              " ['complex analysis', 0],\n",
              " ['holomorphic function', 0],\n",
              " ['NBA season', 0],\n",
              " ['Institute of Design', 0],\n",
              " ['Stuart School', 0],\n",
              " ['phased out', 0],\n",
              " ['phased out its undergraduate', 0],\n",
              " ['out its undergraduate', 0],\n",
              " ['Main Campus', 0],\n",
              " ['per square mile', 0],\n",
              " ['described as', 0],\n",
              " ['Mill Mountain', 0],\n",
              " ['work for', 0],\n",
              " ['MGM Records', 0],\n",
              " ['took part', 0],\n",
              " ['outside Beijing', 0],\n",
              " ['Beijing National', 0],\n",
              " ['Olympic Green', 0],\n",
              " ['events were held', 0],\n",
              " ['other attributes', 0],\n",
              " ['computer graphics', 0],\n",
              " ['the Roman', 0],\n",
              " ['the city', 0],\n",
              " ['it became a center', 0],\n",
              " ['became a center', 0],\n",
              " ['a small', 0],\n",
              " ['when he', 0],\n",
              " ['is believed to have', 0],\n",
              " ['believed to have', 0],\n",
              " ['Gotti Jr', 0],\n",
              " ['acting boss', 0],\n",
              " ['kill himself', 0],\n",
              " ['In November', 0],\n",
              " ['complete set', 0],\n",
              " ['packaged in', 0],\n",
              " ['carbon taxes', 0],\n",
              " ['contract may be', 0],\n",
              " ['depending on', 0],\n",
              " ['use of the photograph', 0],\n",
              " ['Ingres Database', 0],\n",
              " ['Hempfield is', 0],\n",
              " ['fact that', 0],\n",
              " ['new versions', 0],\n",
              " ['South Australian', 0],\n",
              " ['has grown', 0],\n",
              " ['on the campus', 0],\n",
              " ['cor anglais', 0],\n",
              " ['perfect fifth', 0],\n",
              " ['for the cor anglais', 0],\n",
              " ['middle C', 0],\n",
              " ['as a', 0],\n",
              " ['The album was', 0],\n",
              " ['Xipe Totec', 0],\n",
              " ['Swiss Canton', 0],\n",
              " ['State Park', 0],\n",
              " ['under the age', 0],\n",
              " ['years of age', 0],\n",
              " ['years of age or older', 0],\n",
              " ['age or older', 0],\n",
              " ['As of the census', 0],\n",
              " ['families residing', 0],\n",
              " ['families residing in the county', 0],\n",
              " ['residing in the county', 0],\n",
              " ['the county', 0],\n",
              " ['The population density', 0],\n",
              " ['The population density was', 0],\n",
              " ['square mile', 0],\n",
              " ['There were', 0],\n",
              " ['housing units', 0],\n",
              " ['housing units at an', 0],\n",
              " ['housing units at an average density', 0],\n",
              " ['units at an', 0],\n",
              " ['units at an average density', 0],\n",
              " ['an average density', 0],\n",
              " ['racial makeup', 0],\n",
              " ['racial makeup of the county', 0],\n",
              " ['makeup of the county', 0],\n",
              " ['county was', 0],\n",
              " ['Black or African', 0],\n",
              " ['African American', 0],\n",
              " ['Native American', 0],\n",
              " ['Pacific Islander', 0],\n",
              " ['other races', 0],\n",
              " ['two or more', 0],\n",
              " ['two or more races', 0],\n",
              " ['more races', 0],\n",
              " ['of the population were', 0],\n",
              " ['population were Hispanic', 0],\n",
              " ['population were Hispanic or Latino', 0],\n",
              " ['Hispanic or Latino', 0],\n",
              " ['Hispanic or Latino of any', 0],\n",
              " ['Latino of any', 0],\n",
              " ['any race', 0],\n",
              " ['Latino of any race', 0],\n",
              " ['Hispanic or Latino of any race', 0],\n",
              " ['The county is', 0],\n",
              " ['part of the land', 0],\n",
              " ['married couples', 0],\n",
              " ['married couples living together', 0],\n",
              " ['couples living together', 0],\n",
              " ['children under', 0],\n",
              " ['children under the age', 0],\n",
              " ['had a female', 0],\n",
              " ['had a female householder', 0],\n",
              " ['had a female householder with no', 0],\n",
              " ['female householder', 0],\n",
              " ['female householder with no husband', 0],\n",
              " ['female householder with no husband present', 0],\n",
              " ['householder with no', 0],\n",
              " ['no husband present', 0],\n",
              " ['husband present', 0],\n",
              " ['average household size', 0],\n",
              " ['average family size', 0],\n",
              " ['mainframe computers', 0],\n",
              " ['Los Rodeos', 0],\n",
              " ['ground level', 0],\n",
              " ['might come', 0],\n",
              " ['County is a county located', 0],\n",
              " ['located in', 0],\n",
              " ['Its county seat', 0],\n",
              " ['median income', 0],\n",
              " ['median income for a household', 0],\n",
              " ['income for', 0],\n",
              " ['income for a household', 0],\n",
              " ['for a household in the county', 0],\n",
              " ['household in the county', 0],\n",
              " ['median income for a family', 0],\n",
              " ['income for a family', 0],\n",
              " ['Males had', 0],\n",
              " ['Males had a median', 0],\n",
              " ['Males had a median income', 0],\n",
              " ['The per capita income for', 0],\n",
              " ['per capita income', 0],\n",
              " ['per capita income for the county', 0],\n",
              " ['capita income', 0],\n",
              " ['capita income for the county', 0],\n",
              " ['population were below', 0],\n",
              " ['population were below the poverty', 0],\n",
              " ['population were below the poverty line', 0],\n",
              " ['below the poverty', 0],\n",
              " ['below the poverty line', 0],\n",
              " ['poverty line', 0],\n",
              " ['those under', 0],\n",
              " ['size of the strike', 0],\n",
              " ['size of the strike zone', 0],\n",
              " ['strike zone', 0],\n",
              " ['tended to be', 0],\n",
              " ['standard gauge', 0],\n",
              " ['railway station', 0],\n",
              " ['Star Brewery', 0],\n",
              " ['James East', 0],\n",
              " [\"Tender's fame\", 0],\n",
              " ['county population', 0],\n",
              " ['his twin sister', 0],\n",
              " ['lay with his', 0],\n",
              " ['sister and she', 0],\n",
              " ['sister and she bore', 0],\n",
              " ['sister and she bore him', 0],\n",
              " ['she bore him', 0],\n",
              " ['bore him', 0],\n",
              " ['bore him a daughter', 0],\n",
              " ['him a daughter', 0],\n",
              " ['households out of which', 0],\n",
              " ['out of which', 0],\n",
              " ['had children under the age', 0],\n",
              " ['householder with no husband', 0],\n",
              " ['all households', 0],\n",
              " ['all households were made', 0],\n",
              " ['all households were made up', 0],\n",
              " ['households were made', 0],\n",
              " ['households were made up of individuals', 0],\n",
              " ['made up', 0],\n",
              " ['made up of individuals', 0],\n",
              " ['up of individuals', 0],\n",
              " ['someone living alone', 0],\n",
              " ['someone living alone who', 0],\n",
              " ['alone who', 0],\n",
              " ['years of age or', 0],\n",
              " ['The average household', 0],\n",
              " ['The average household size', 0],\n",
              " ['ranged from', 0],\n",
              " ['January to', 0],\n",
              " ['in July', 0],\n",
              " ['there were', 0],\n",
              " ['an American', 0],\n",
              " ['news of her abduction', 0],\n",
              " ['her abduction', 0],\n",
              " ['negative ads', 0],\n",
              " ['negative ads against Riordan', 0],\n",
              " ['ads against Riordan', 0],\n",
              " ['regular numbers', 0],\n",
              " ['Deep Fritz', 0],\n",
              " ['opening book', 0],\n",
              " ['who had', 0],\n",
              " ['then known as the Global', 0],\n",
              " [\"then known as the Global Guardians'\", 0],\n",
              " [\"Global Guardians'\", 0],\n",
              " ['connected between switching', 0],\n",
              " ['connected between switching exchanges using', 0],\n",
              " ['switching exchanges', 0],\n",
              " ['connection is called', 0],\n",
              " ['historical cost', 0],\n",
              " [\"magistrates' court\", 0],\n",
              " ['smaller cities', 0],\n",
              " ['it was the first', 0],\n",
              " ['ethnic Romanians', 0],\n",
              " ['UW Colleges', 0],\n",
              " ['associate degrees', 0],\n",
              " ['Everly Brothers', 0],\n",
              " ['Two Islands', 0],\n",
              " ['Islands Reef', 0],\n",
              " ['Three Islands', 0],\n",
              " ['and Camp Oest', 0],\n",
              " ['Hartland Connecticut', 0],\n",
              " ['The population', 0],\n",
              " ['A new', 0],\n",
              " ['Hong Kong', 0],\n",
              " ['educational attainment', 0],\n",
              " ['from two or more races', 0],\n",
              " ['of the population', 0],\n",
              " ['The median', 0],\n",
              " ['The median income for a household', 0],\n",
              " ['median income for a', 0],\n",
              " ['for a household', 0],\n",
              " ['for a household in the CDP', 0],\n",
              " ['household in the CDP', 0],\n",
              " ['and the median', 0],\n",
              " ['and the median income for', 0],\n",
              " ['for a family', 0],\n",
              " ['per capita', 0],\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    }
  ]
}