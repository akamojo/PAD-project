{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akamojo/PAD-project/blob/master/part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvo76VXTnwmp",
        "colab_type": "text"
      },
      "source": [
        "# PAD 2nd Assignment PART 1\n",
        "- Mitchell Galvao MIEI 41646\n",
        "- Urszula Walińska MIEI 56556\n",
        "\n",
        "## Implementation of  the  multi‐word  Relevant  Expressions  LocalMaxs  extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx9U5s-q36iQ",
        "colab_type": "code",
        "outputId": "9da0df1d-e301-452b-c09a-0ce8b1257d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[?25l  Downloading https://github.com/chengs/tqdm/archive/colab.zip (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 764kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: tqdm\n",
            "  Building wheel for tqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o15yijb4/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.28.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FfGi6IV3wHcr",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rffrtUAUwJR-",
        "outputId": "02ed301a-fd19-43d7-9e5f-575c2362fbbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85ttuBRjolJ4",
        "colab_type": "text"
      },
      "source": [
        "## Loading 2 mln corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "17-cUaXGwHc1",
        "colab": {}
      },
      "source": [
        "file = open(\"/content/drive/My Drive/STUDIA/SEM 8/pad/en1.8m.txt\", \"r\", encoding=\"utf8\")\n",
        "#file = open(\"en1.8m.txt\", \"r\", encoding=\"utf8\")\n",
        "\n",
        "whole_text = file.read()\n",
        "text = whole_text.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdfRh6HEo6v1",
        "colab_type": "text"
      },
      "source": [
        "## Data pre-processing  \n",
        "Separation from words the characters that do not change the text semantics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9PHsE2zqwHdE",
        "colab": {}
      },
      "source": [
        "def append_space(text, pattern, left=True):\n",
        "    p = re.compile(\"[\" + pattern + \"]\")\n",
        "    \n",
        "    for i, m in enumerate(p.finditer(text)):\n",
        "        if left:\n",
        "            text = text[: m.start() + i] + \" \" + text[m.start() + i :]\n",
        "        else:\n",
        "            text = text[: m.start() + 1 + i] + \" \" + text[m.start() + 1 + i :]\n",
        "            \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0oVrjurXwHdQ",
        "colab": {}
      },
      "source": [
        "def preprocessing(text):\n",
        "    for i in range(len(text)):\n",
        "        text[i] = append_space(text[i], \",\\])>\")\n",
        "        text[i] = append_space(text[i], \".\")\n",
        "        text[i] = append_space(text[i], \"\\[(<\", False)\n",
        "      \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FrB2jwJpMvo",
        "colab_type": "text"
      },
      "source": [
        "## Function for building n-grams  \n",
        "Based on the list of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9x-KrFiNwHdU",
        "colab": {}
      },
      "source": [
        "def build_n_gram(text_arr, n_gram, grams):\n",
        "    cur_gram = []\n",
        "    count_gram = 0\n",
        "\n",
        "    for i in range(len(text_arr)):\n",
        "\n",
        "        cur_gram = [text_arr[i]]\n",
        "\n",
        "        for j in range(i + 1, i + n_gram):\n",
        "            if j < len(text_arr):\n",
        "                cur_gram.append(text_arr[j])\n",
        "\n",
        "        if len(cur_gram) == n_gram:\n",
        "            try:\n",
        "                grams[\" \".join(cur_gram)] += 1\n",
        "            except KeyError:\n",
        "                grams[\" \".join(cur_gram)] = 1\n",
        "\n",
        "    return grams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm4ANJ5-pcFv",
        "colab_type": "text"
      },
      "source": [
        "## Building dictionary  \n",
        "Which consists of n-grams and their frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pkr_cvxVwHdd",
        "colab": {}
      },
      "source": [
        "def build_dict(text, n_s):\n",
        "    dictionary = []\n",
        "    for n in range(n_s):\n",
        "        dictionary.append(dict())\n",
        "    \n",
        "    for n in range(n_s):\n",
        "        for line in text:\n",
        "            dictionary[n] = build_n_gram(line.split(), n + 1, dictionary[n])\n",
        "        \n",
        "    return dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i2ebntCpjp2",
        "colab_type": "text"
      },
      "source": [
        "## SCP_F measure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZrO6W8pwHdg",
        "colab": {}
      },
      "source": [
        "def SCP_f(words, dictionary):\n",
        "    words_arr = words.split()\n",
        "    \n",
        "    numerator = dictionary[len(words_arr) - 1][words] ** 2\n",
        "    F = 1 / (len(words_arr) - 1)\n",
        "    denominator = 0\n",
        "    \n",
        "    for i in range(len(words_arr) - 1):\n",
        "        left = words_arr[0:i+1]\n",
        "        right = words_arr[i+1:]\n",
        "        \n",
        "        denominator += dictionary[len(left) - 1][\" \".join(left)] * dictionary[len(right) - 1][\" \".join(right)]\n",
        "        \n",
        "    F *= denominator\n",
        "    return numerator / F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg78ws-upmVW",
        "colab_type": "text"
      },
      "source": [
        "## Dice measure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs4Uhmut6fSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dice(words, dictionary):\n",
        "    words_arr = words.split()\n",
        "    \n",
        "    numerator = dictionary[len(words_arr) - 1][words] * 2\n",
        "    F = 1 / (len(words_arr) - 1)\n",
        "    denominator = 0\n",
        "    \n",
        "    for i in range(len(words_arr) - 1):\n",
        "        left = words_arr[0:i+1]\n",
        "        right = words_arr[i+1:]\n",
        "        \n",
        "        denominator += dictionary[len(left) - 1][\" \".join(left)] + dictionary[len(right) - 1][\" \".join(right)]\n",
        "        \n",
        "    F *= denominator\n",
        "    return numerator / F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5st2-BoppdH",
        "colab_type": "text"
      },
      "source": [
        "## LocalMax algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFt8PTVOESoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LocalMax(text, table):\n",
        "    xys = dict()\n",
        "    \n",
        "    for nr in tqdm(range(len(text))):\n",
        "#     for nr in range(len(text)):\n",
        "        line = text[nr]\n",
        "\n",
        "        text_arr = line.split()\n",
        "\n",
        "        if len(text_arr) < 2:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(text_arr) - 2):\n",
        "            cur_gram = \" \".join(text_arr[i:i+2])\n",
        "\n",
        "            j = i + 3\n",
        "            n_gram = 1\n",
        "\n",
        "            while j < len(text_arr) and n_gram < 6:\n",
        "                next_gram = \" \".join(text_arr[i:j])\n",
        "\n",
        "                try:\n",
        "                    xys[cur_gram][1] = max(xys[cur_gram][1], table[next_gram])\n",
        "                    \n",
        "                except KeyError:\n",
        "                    if len(cur_gram.split()) > 2:\n",
        "                        x = table[\" \".join(cur_gram.split()[:-1])]\n",
        "                        x = max(x, table[\" \".join(cur_gram.split()[1:])])\n",
        "                        xys[cur_gram] = [x, table[next_gram]]\n",
        "                    else:\n",
        "                        xys[cur_gram] = [-1, table[next_gram]]\n",
        "\n",
        "                cur_gram = next_gram\n",
        "\n",
        "                j += 1\n",
        "                n_gram += 1\n",
        "\n",
        "            cur_gram = \" \".join(text_arr[i:i+2])\n",
        "            j = i - 1\n",
        "            n_gram = 1\n",
        "\n",
        "            while j >= 0 and n_gram < 6:\n",
        "                next_gram = \" \".join(text_arr[j:i+2])\n",
        "\n",
        "\n",
        "                try:                        \n",
        "                    xys[cur_gram][1] = max(xys[cur_gram][1], table[next_gram])\n",
        "                    \n",
        "                except KeyError:\n",
        "                    if len(cur_gram.split()) > 2:\n",
        "                        x = table[\" \".join(cur_gram.split()[:-1])]\n",
        "                        x = max(x, table[\" \".join(cur_gram.split()[1:])])\n",
        "                        xys[cur_gram] = [x, table[next_gram]]\n",
        "                    else:\n",
        "                        xys[cur_gram] = [-1, table[next_gram]]\n",
        "\n",
        "\n",
        "                cur_gram = next_gram\n",
        "\n",
        "                j -= 1\n",
        "                n_gram += 1\n",
        "              \n",
        "    return xys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtRwJk-opvuW",
        "colab_type": "text"
      },
      "source": [
        "## Main function  \n",
        "In this function the text passed as an argument of the function is pre-processed first. Than the dictionary with n-grams and frequencies is build. Next the desired glue value of each n-gram is calculated, in order to be able to apply LocalMax algorithm and find Relevant Expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0wd9fUsowHdq",
        "colab": {}
      },
      "source": [
        "def main(text, max_gram, threshold, glue_fun, verbose=False):\n",
        "    if verbose:\n",
        "        print(\"Preprocessing text\")\n",
        "    text = preprocessing(text)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Building dictionary with frequencies\")\n",
        "\n",
        "    dictionary = build_dict(text, max_gram)\n",
        "    \n",
        "    table = dict()\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Proceeding to calculating glue value\")\n",
        "        print(\"Glue value calculated for:\")\n",
        "\n",
        "    for i in range(2, max_gram + 1):\n",
        "        if verbose:\n",
        "            print(i, \"grams\")\n",
        "            \n",
        "        for s in dictionary[i - 1]:\n",
        "          table[s] = glue_fun(s, dictionary)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Proceeding to Localmax\")\n",
        "    \n",
        "    RE = []\n",
        "    xys = LocalMax(text, table)\n",
        "            \n",
        "    for g in xys:\n",
        "        if xys[g][0] == -1:\n",
        "            xys[g][0] = xys[g][1]\n",
        "        \n",
        "        val = (xys[g][0] + xys[g][1]) / 2\n",
        "        \n",
        "        if table[g] > val and re.match(r\"^[a-z\\'A-Z\\s]*$\", g) and dictionary[len(g.split()) - 1][g] > threshold:\n",
        "            RE.append(g)\n",
        "    \n",
        "    return RE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5dc4Lafqqql",
        "colab_type": "text"
      },
      "source": [
        "## Result of our method for the first paragraph of the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MGuj-XKEl89",
        "colab_type": "code",
        "outputId": "332f24a2-f13d-47c9-ad1f-acefe5592606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "test = text[0]\n",
        "\n",
        "RE = main([test], 7, 1, SCP_f)\n",
        "print(\"Obtained\", len(RE), \"Relevant Expressions\")\n",
        "RE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm_notebook.py:88: TqdmExperimentalWarning: Detect Google Colab 0.0.1a2 and thus load dummy ipywidgets package. Note that UI is different from that in Jupyter. See https://github.com/tqdm/tqdm/pull/640\n",
            "  \" See https://github.com/tqdm/tqdm/pull/640\".format(colab.__version__), TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='1' value='1'></progress>100% 1/1 [00:00&lt;00:00, 45.04it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Obtained 2 Relevant Expressions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in the', 'John Philoponus']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVO0Io_zqxx-",
        "colab_type": "text"
      },
      "source": [
        "## Results for the whole corpus  \n",
        "As we can see, for the corpus of 2 mln words size, using SCP_f function as glue measure, having n-grams of the maximum size of 7 words and threshold for the minimum frequency of Relevant Expression equal 1, we obtain 19059 Relevant Expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtcH06Du7TeP",
        "colab_type": "code",
        "outputId": "4f48ee82-d262-4e05-e38c-b4a663162b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "RE = main(text, 7, 1, SCP_f, True)\n",
        "print(\"Obtained\", len(RE), \"Relevant Expressions\")\n",
        "results = RE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing text\n",
            "Building dictionary with frequencies\n",
            "Proceeding to calculating glue value\n",
            "Glue value calculated for:\n",
            "2 grams\n",
            "3 grams\n",
            "4 grams\n",
            "5 grams\n",
            "6 grams\n",
            "7 grams\n",
            "Proceeding to Localmax\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='69468' value='69468'></progress>100% 69468/69468 [00:55&lt;00:00, 1254.42it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Obtained 19059 Relevant Expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZN044pruRr_",
        "colab_type": "code",
        "outputId": "2e7632f7-838c-4beb-a127-6ca043a66474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        }
      },
      "source": [
        "RE[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['crucial role',\n",
              " 'in the',\n",
              " 'of the',\n",
              " 'The first',\n",
              " 'John Philoponus',\n",
              " 'on the',\n",
              " 'and other',\n",
              " 'communications equipment',\n",
              " 'based on',\n",
              " 'with the',\n",
              " 'divided into',\n",
              " 'the use of',\n",
              " 'the study of',\n",
              " 'known as',\n",
              " 'what is now',\n",
              " 'from the',\n",
              " 'to the',\n",
              " 'as a',\n",
              " 'was not',\n",
              " 'end of',\n",
              " 'by the',\n",
              " 'of their',\n",
              " 'After independence',\n",
              " 'civil war',\n",
              " 'such as',\n",
              " 'the dawn of',\n",
              " 'the dawn of the',\n",
              " 'Ottoman Empire',\n",
              " 'the geopolitical',\n",
              " 'He also',\n",
              " 'for the',\n",
              " 'western Europe',\n",
              " 'as well as',\n",
              " 'in the form of',\n",
              " 'is a',\n",
              " 'Indic scripts',\n",
              " 'Southeast Asia',\n",
              " 'they are',\n",
              " 'Northern India',\n",
              " 'Sri Lanka',\n",
              " 'the site of',\n",
              " 'Charles VI of France',\n",
              " 'VI of France',\n",
              " 'as the',\n",
              " 'According to',\n",
              " 'that they',\n",
              " 'they were',\n",
              " 'the start of',\n",
              " 'start of the battle',\n",
              " 'to be',\n",
              " 'formula for calculating',\n",
              " 'is not',\n",
              " 'Western Church',\n",
              " 'abbots of monasteries',\n",
              " 'to a',\n",
              " 'as an',\n",
              " 'Roman Catholic',\n",
              " 'Catholic Church',\n",
              " 'Orthodox Church',\n",
              " 'are permitted to be',\n",
              " 'the rank of',\n",
              " 'there are',\n",
              " 'are not',\n",
              " 'have been',\n",
              " 'Catherine II',\n",
              " 'the ranks of',\n",
              " 'may be',\n",
              " 'may be given to any',\n",
              " 'does not',\n",
              " 'of a',\n",
              " 'head of a monastery',\n",
              " 'celibate priest',\n",
              " 'World War II',\n",
              " 'their respective',\n",
              " 'respective countries',\n",
              " 'Olympic Games',\n",
              " 'Summer Olympics',\n",
              " 'the development of',\n",
              " 'one of',\n",
              " 'one of the most',\n",
              " 'located in',\n",
              " 'Karaiskakis Stadium',\n",
              " 'sports and entertainment',\n",
              " 'UEFA Cup',\n",
              " \"Cup Winners' Cup\",\n",
              " \"Winners' Cup Final\",\n",
              " 'The current',\n",
              " 'at the',\n",
              " 'at the University of',\n",
              " 'United States',\n",
              " 'and the',\n",
              " 'he has',\n",
              " 'has been',\n",
              " 'literary criticism',\n",
              " 'With the advent',\n",
              " 'they had',\n",
              " 'Baroque period',\n",
              " 'because it',\n",
              " 'of his',\n",
              " 'the murder of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiVDsvKD2z3v",
        "colab_type": "text"
      },
      "source": [
        "## Possible improvement  \n",
        "As we can see in the example above we extracted 100 Relevant Expression from the 2 milion corpus text, Although some of them shouldn't be considered as Relevant Expression. For example \"in the\" or \"at the\" shouldn't be considered Relevant Expressions since they are very common words that wont give us great information about the meaning of the text.\n",
        "As a possible improvement of the method, we propose filtering the list of Relevant Expressions, rejecting those that start with a stop word. For that we have a text file named stopwords.txt with very common words that we can exclude from the Relevant Expressions Extrator giving us a more concrete information about the text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgWWft0u2xLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open(\"/content/drive/My Drive/STUDIA/SEM 8/pad/stopwords.txt\", \"r\")\n",
        "stop_words = file.read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gysDS10H3ttJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def begins_with_stopword(expr):\n",
        "    first_word = expr.split()[0].lower()\n",
        "    return first_word in stop_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHnIoLqU3Qk-",
        "colab_type": "code",
        "outputId": "cf9db354-f4fa-49fc-ae9c-cb5fcfcffaca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        }
      },
      "source": [
        "RE_without_stopwords = list(filter(lambda x: not begins_with_stopword(x), RE))\n",
        "RE_without_stopwords[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['crucial role',\n",
              " 'John Philoponus',\n",
              " 'communications equipment',\n",
              " 'based on',\n",
              " 'divided into',\n",
              " 'known as',\n",
              " 'end of',\n",
              " 'civil war',\n",
              " 'Ottoman Empire',\n",
              " 'western Europe',\n",
              " 'Indic scripts',\n",
              " 'Southeast Asia',\n",
              " 'Northern India',\n",
              " 'Sri Lanka',\n",
              " 'Charles VI of France',\n",
              " 'VI of France',\n",
              " 'According to',\n",
              " 'start of the battle',\n",
              " 'formula for calculating',\n",
              " 'Western Church',\n",
              " 'abbots of monasteries',\n",
              " 'Roman Catholic',\n",
              " 'Catholic Church',\n",
              " 'Orthodox Church',\n",
              " 'Catherine II',\n",
              " 'may be',\n",
              " 'may be given to any',\n",
              " 'head of a monastery',\n",
              " 'celibate priest',\n",
              " 'World War II',\n",
              " 'respective countries',\n",
              " 'Olympic Games',\n",
              " 'Summer Olympics',\n",
              " 'one of',\n",
              " 'one of the most',\n",
              " 'located in',\n",
              " 'Karaiskakis Stadium',\n",
              " 'sports and entertainment',\n",
              " 'UEFA Cup',\n",
              " \"Cup Winners' Cup\",\n",
              " \"Winners' Cup Final\",\n",
              " 'United States',\n",
              " 'literary criticism',\n",
              " 'Baroque period',\n",
              " 'murder of Naboth',\n",
              " 'denunciation of',\n",
              " 'leading figure',\n",
              " 'found in',\n",
              " 'associated with',\n",
              " 'Albertus Magnus',\n",
              " 'referred to him as',\n",
              " 'Middle Ages',\n",
              " 'Doctor of the Church',\n",
              " 'United Kingdom',\n",
              " 'New Orleans',\n",
              " 'Battle of New Orleans',\n",
              " 'decisive victory',\n",
              " 'part of',\n",
              " 'took their place',\n",
              " 'Pope Honorius',\n",
              " 'depart for the Holy Land',\n",
              " 'Steven Runciman',\n",
              " 'Peter of Courtenay',\n",
              " 'second and third',\n",
              " 'due to',\n",
              " 'Kennedy Space Center',\n",
              " 'first time',\n",
              " 'designed to operate',\n",
              " 'surface of the Earth',\n",
              " 'entire bay',\n",
              " 'test site',\n",
              " 'end of World War',\n",
              " 'end of World War II',\n",
              " 'fell in love',\n",
              " 'entire island',\n",
              " 'Atlantic Ocean',\n",
              " 'surrounding area',\n",
              " 'may refer',\n",
              " 'written by',\n",
              " 'King of the Romans',\n",
              " 'Upper Rhine',\n",
              " 'led to a great deal',\n",
              " 'great deal',\n",
              " 'political upheaval',\n",
              " 'number of',\n",
              " 'production processes',\n",
              " 'electrical engineering',\n",
              " 'East Franks',\n",
              " 'Louis the Child',\n",
              " 'ceased to exist',\n",
              " 'population pyramid',\n",
              " 'megabytes per',\n",
              " 'megabytes per second',\n",
              " 'sends information',\n",
              " 'removal of a hydrogen',\n",
              " 'removal of a hydrogen ion',\n",
              " 'hydrogen ion',\n",
              " 'Pope Pius',\n",
              " 'oxidative stress',\n",
              " 'Muddy Waters']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6hFNcgUfi1c",
        "colab_type": "text"
      },
      "source": [
        "As we can see comparing the 100 Relevant expressions extrated with our stopwords.txt file with the Relevant expressions extrated without the file we can observe that the Relevant expressions extracted with the file are much more informative and give us a better knowledge of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCMsW-anrgkm",
        "colab_type": "text"
      },
      "source": [
        "##Using Dice Measure\n",
        "On the contrary, using dice measure (leaving values of the rest of parameters the same) we obtain 38124 Relevant Expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLzfnTVJI1rG",
        "colab_type": "code",
        "outputId": "b2a176c4-578b-45a4-b958-f31f1c5b339a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "RE = main(text, 7, 1, dice)\n",
        "print(\"Obtained\", len(RE), \"Relevant Expressions\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing text\n",
            "Building dictionary with frequencies\n",
            "Proceeding to calculating glue value\n",
            "Glue value calculated for:\n",
            "2 grams\n",
            "3 grams\n",
            "4 grams\n",
            "5 grams\n",
            "6 grams\n",
            "7 grams\n",
            "Proceeding to Localmax\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='69468' value='69468'></progress>100% 69468/69468 [00:51&lt;00:00, 1347.27it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Obtained 38124 Relevant Expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njq6NL6Fr0fz",
        "colab_type": "text"
      },
      "source": [
        "Below, exemplary 100 obtained Relevant Expressions are presented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNynzPSz5tYI",
        "colab_type": "code",
        "outputId": "0569060f-6a0c-468d-b45c-d67712efe78b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        }
      },
      "source": [
        "RE[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['crucial role',\n",
              " 'in the',\n",
              " 'manuscripts of',\n",
              " 'of the',\n",
              " 'The first',\n",
              " 'John Philoponus',\n",
              " 'out for',\n",
              " 'on the',\n",
              " 'and other',\n",
              " 'other elements',\n",
              " 'several centuries',\n",
              " 'communications equipment',\n",
              " 'based on',\n",
              " 'The wording',\n",
              " 'with the',\n",
              " 'divided into',\n",
              " 'or gross',\n",
              " 'is the',\n",
              " 'of an',\n",
              " 'body parts',\n",
              " 'also includes',\n",
              " 'use of',\n",
              " 'known as',\n",
              " 'and also',\n",
              " 'also in',\n",
              " 'what is now',\n",
              " 'from the',\n",
              " 'to the',\n",
              " 'lived there',\n",
              " 'as a',\n",
              " 'was not',\n",
              " 'before the end',\n",
              " 'by the',\n",
              " 'only by',\n",
              " 'After independence',\n",
              " 'civil war',\n",
              " 'areas such',\n",
              " 'such as',\n",
              " 'which have',\n",
              " 'have included',\n",
              " 'included the former',\n",
              " 'dawn of',\n",
              " 'establishment of the Ottoman',\n",
              " 'Ottoman Empire',\n",
              " 'The Ottomans',\n",
              " 'over most',\n",
              " 'many times',\n",
              " 'led by',\n",
              " 'and later',\n",
              " 'later established',\n",
              " 'He also',\n",
              " 'which they',\n",
              " 'for the',\n",
              " 'western Europe',\n",
              " 'as well',\n",
              " 'as well as',\n",
              " 'in the form of',\n",
              " 'is a',\n",
              " 'Indic scripts',\n",
              " 'Southeast Asia',\n",
              " 'Today they',\n",
              " 'they are',\n",
              " 'are used',\n",
              " 'used in',\n",
              " 'South Asia',\n",
              " 'some other',\n",
              " 'mainland Southeast',\n",
              " 'but not',\n",
              " 'The primary',\n",
              " 'Northern India',\n",
              " 'South India',\n",
              " 'Sri Lanka',\n",
              " 'near the site',\n",
              " 'in which',\n",
              " 'which the',\n",
              " 'army led',\n",
              " 'King Henry',\n",
              " 'defeated the forces',\n",
              " 'forces led',\n",
              " 'Charles VI',\n",
              " 'Charles VI of France',\n",
              " 'VI of France',\n",
              " 'which has',\n",
              " 'gone down',\n",
              " 'as the',\n",
              " 'According to',\n",
              " 'by their',\n",
              " 'that they',\n",
              " 'they were',\n",
              " 'even before',\n",
              " 'before the start',\n",
              " 'the start of the',\n",
              " 'start of the battle',\n",
              " 'was a',\n",
              " 'that could',\n",
              " 'but which',\n",
              " 'them to',\n",
              " 'to be',\n",
              " 'he found',\n",
              " 'it only']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmAbKRgSr9yD",
        "colab_type": "text"
      },
      "source": [
        "## Using the extractor in corpus of 4 million words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-YJHSl4wGNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open(\"/content/drive/My Drive/STUDIA/SEM 8/pad/en3.67m.txt\", \"r\", encoding=\"utf8\")\n",
        "\n",
        "whole_text = file.read()\n",
        "text = whole_text.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPkeds0p6G3l",
        "colab_type": "code",
        "outputId": "4891917e-3be4-4539-afc4-75944c993451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "RE = main(text, 7, 1, SCP_f)\n",
        "print(\"Obtained\", len(RE), \"Relevant Expressions\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing text\n",
            "Building dictionary with frequencies\n",
            "Proceeding to calculating glue value\n",
            "Glue value calculated for:\n",
            "2 grams\n",
            "3 grams\n",
            "4 grams\n",
            "5 grams\n",
            "6 grams\n",
            "7 grams\n",
            "Proceeding to Localmax\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='138936' value='138936'></progress>100% 138936/138936 [01:48&lt;00:00, 1276.31it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Obtained 38351 Relevant Expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_21j53cgsfkq",
        "colab_type": "text"
      },
      "source": [
        "Extracting Relevant Expressions from the corpus of the size 4 mln words lasted approximately 5 minutes and as a result 38351 Relevant Expressions were obtained. Values of parameters: maximal length of n-gram = 7, minimal frequency threshold of RE = 1, glue measure = SCP_f.  \n",
        "Below, exemplary 100 REs are presented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9EoOP-YyISe",
        "colab_type": "code",
        "outputId": "0e770977-565d-4ac4-8dfd-ea6d92af5994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        }
      },
      "source": [
        "RE[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alcoholic beverage',\n",
              " 'that the',\n",
              " 'on the',\n",
              " 'the sale of',\n",
              " 'alcoholic beverages',\n",
              " 'in the',\n",
              " 'of the',\n",
              " 'there are',\n",
              " 'crucial role',\n",
              " 'The first',\n",
              " 'John Philoponus',\n",
              " 'and other',\n",
              " 'part of',\n",
              " 'in the northern part of',\n",
              " 'northern part',\n",
              " 'where it',\n",
              " 'as much as',\n",
              " 'communications equipment',\n",
              " 'based on',\n",
              " 'The assumption',\n",
              " 'the same',\n",
              " 'with the',\n",
              " 'with a',\n",
              " 'with an',\n",
              " 'an apogee',\n",
              " 'and a',\n",
              " 'to the',\n",
              " 'orbital period',\n",
              " 'divided into',\n",
              " 'of an',\n",
              " 'the use of',\n",
              " 'the study of',\n",
              " 'known as',\n",
              " 'according to',\n",
              " 'the second',\n",
              " 'in terms of',\n",
              " 'what is now',\n",
              " 'from the',\n",
              " 'began to',\n",
              " 'as a',\n",
              " 'was not',\n",
              " 'end of',\n",
              " 'by the',\n",
              " 'after the',\n",
              " 'of their',\n",
              " 'civil war',\n",
              " 'such as',\n",
              " 'decreasing order',\n",
              " 'most common',\n",
              " 'for the',\n",
              " 'number of',\n",
              " 'and the',\n",
              " 'the dawn of',\n",
              " 'the establishment of',\n",
              " 'Ottoman Empire',\n",
              " 'Southeast Europe',\n",
              " 'most of',\n",
              " 'led by',\n",
              " 'He also',\n",
              " 'which they',\n",
              " 'as well as',\n",
              " 'in the form of',\n",
              " 'his father',\n",
              " 'In the same year',\n",
              " 'same year',\n",
              " 'at the',\n",
              " 'After leaving',\n",
              " 'he became',\n",
              " 'became a',\n",
              " 'During the First World',\n",
              " 'During the First World War',\n",
              " 'military service',\n",
              " 'because of',\n",
              " 'of his',\n",
              " 'have been',\n",
              " 'by a',\n",
              " 'young man',\n",
              " 'up to',\n",
              " 'to a',\n",
              " 'which he',\n",
              " 'is a',\n",
              " 'have a',\n",
              " 'more than',\n",
              " 'a few',\n",
              " 'Indic scripts',\n",
              " 'Southeast Asia',\n",
              " 'they are',\n",
              " 'used in',\n",
              " 'replaced by',\n",
              " 'but not',\n",
              " 'Sri Lanka',\n",
              " 'the beginning of',\n",
              " 'moved to',\n",
              " 'to become',\n",
              " 'the site of',\n",
              " 'Charles VI',\n",
              " 'Charles VI of France',\n",
              " 'which has',\n",
              " 'as the',\n",
              " 'According to']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBvlko1fuYnf",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating the results of the extractor through the Precision, Recall and F-metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0AXf4WRukTX",
        "colab_type": "text"
      },
      "source": [
        "### Precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkKs3CHFuogw",
        "colab_type": "code",
        "outputId": "cd3b3767-fe7d-4bee-cf76-ccd1b14efce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3484
        }
      },
      "source": [
        "from random import randint\n",
        "\n",
        "count = 0\n",
        "\n",
        "for i in range(100):\n",
        "    print(results[randint(0, len(results))])\n",
        "    verdict = input()\n",
        "    \n",
        "    if verdict == 'y':\n",
        "        count += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logitech Media Server\n",
            "y\n",
            "name of several inhabited\n",
            "n\n",
            "Caspian Sea\n",
            "y\n",
            "nausea and vomiting\n",
            "y\n",
            "Jamia Nizamia\n",
            "y\n",
            "Daniel Akaka\n",
            "y\n",
            "westbound platform\n",
            "y\n",
            "largely inhabited\n",
            "n\n",
            "Vocational Education\n",
            "y\n",
            "paternal grandfather\n",
            "y\n",
            "entire field\n",
            "n\n",
            "first TV spot\n",
            "n\n",
            "States and Europe\n",
            "n\n",
            "South Africa\n",
            "y\n",
            "drug pellets\n",
            "y\n",
            "The movie ends\n",
            "n\n",
            "graduated from high school\n",
            "y\n",
            "Governor General of Canada\n",
            "y\n",
            "court to order\n",
            "n\n",
            "another Christian church\n",
            "n\n",
            "marriage is to be dissolved\n",
            "n\n",
            "Its primary mission\n",
            "n\n",
            "sopra Minerva\n",
            "y\n",
            "Queensland Heritage Register\n",
            "y\n",
            "charting singles\n",
            "y\n",
            "pots and pans\n",
            "y\n",
            "He died in Paris\n",
            "n\n",
            "Dean of the College\n",
            "y\n",
            "Tortricidae family\n",
            "y\n",
            "Judicial Commissioners\n",
            "y\n",
            "top seller\n",
            "y\n",
            "Lala Lajpat\n",
            "y\n",
            "California legislature\n",
            "y\n",
            "capacitive sensing\n",
            "y\n",
            "playing professionally\n",
            "n\n",
            "involved some significant events\n",
            "n\n",
            "convexity assumptions\n",
            "y\n",
            "converted to a torpedo\n",
            "y\n",
            "Asia Pacific\n",
            "y\n",
            "Washington DC\n",
            "y\n",
            "Peter Debruge\n",
            "y\n",
            "Freiherr of Weissenau\n",
            "y\n",
            "broad categories\n",
            "y\n",
            "La Plata\n",
            "y\n",
            "she also starred\n",
            "n\n",
            "Black Bolt\n",
            "y\n",
            "Upper West Side\n",
            "y\n",
            "since been demolished\n",
            "n\n",
            "protein isoforms\n",
            "y\n",
            "charges filed against\n",
            "n\n",
            "William Rosewell\n",
            "y\n",
            "natural flow\n",
            "y\n",
            "ISU Speed Skating World Cup\n",
            "y\n",
            "black man\n",
            "y\n",
            "One example\n",
            "n\n",
            "In recent centuries\n",
            "n\n",
            "Olas Boulevard\n",
            "y\n",
            "Wholesale Sports\n",
            "y\n",
            "Series of Poker\n",
            "y\n",
            "used the site as\n",
            "n\n",
            "Liberal Arts\n",
            "y\n",
            "Monolith Soft\n",
            "y\n",
            "Iraq and Afghanistan\n",
            "y\n",
            "An attempt was made\n",
            "n\n",
            "rare cases\n",
            "n\n",
            "but died before\n",
            "n\n",
            "short distance\n",
            "y\n",
            "passengers travelling\n",
            "y\n",
            "a Captain in\n",
            "n\n",
            "physical or intellectual\n",
            "y\n",
            "president of the Royal Astronomical\n",
            "y\n",
            "Sonoma County\n",
            "y\n",
            "on the occasion of\n",
            "n\n",
            "Systems Biology\n",
            "y\n",
            "UEFA Cup\n",
            "y\n",
            "awarded the Medal\n",
            "y\n",
            "went on to win\n",
            "n\n",
            "Mental Research Institute\n",
            "y\n",
            "ITV network\n",
            "y\n",
            "Enix Music Online\n",
            "y\n",
            "their newest album\n",
            "n\n",
            "Republic of the Congo\n",
            "y\n",
            "strong influence\n",
            "y\n",
            "Tynisha Keli\n",
            "y\n",
            "lateral geniculate nucleus\n",
            "y\n",
            "List of Prokaryotic names\n",
            "y\n",
            "Promised Day Brigade\n",
            "y\n",
            "written in one column per page\n",
            "n\n",
            "generator transformer\n",
            "y\n",
            "Mediterranean Sea\n",
            "y\n",
            "few months\n",
            "n\n",
            "In his first year\n",
            "n\n",
            "Royal Shakespeare Company\n",
            "y\n",
            "Diana Ross\n",
            "y\n",
            "weak tropical low had formed\n",
            "n\n",
            "Arjuna Awardee\n",
            "y\n",
            "social worker\n",
            "y\n",
            "Hurler of\n",
            "n\n",
            "hundreds of victims\n",
            "y\n",
            "Carl Kempe\n",
            "y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQp8EYDBvY5l",
        "colab_type": "code",
        "outputId": "6570e2fa-d74a-4613-d2a6-a4dc01df7640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "FP = 100 - count\n",
        "TP = count\n",
        "precision = TP / (FP + TP)\n",
        "print('Precision: {} %'.format(100 * precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 69.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rud6WiTI9z9N",
        "colab_type": "text"
      },
      "source": [
        "Taking into consideration 100 randomly chosen Relevant Expressions from the ones extracted by our algorithm from the whole 2 mln words corpus, we can say that 31 of them were incorrectly classified as REs and as a result our method reached 69% of precision. In our opinion, for example, 'few months' or 'Hurler of' cannot be considered as REs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfBcsG2Feqq7",
        "colab_type": "code",
        "outputId": "2cd443ca-5320-40ec-ca98-ebf89cb1af4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3484
        }
      },
      "source": [
        "count = 0\n",
        "\n",
        "for i in range(100):\n",
        "    print(RE_without_stopwords[randint(0, len(RE_without_stopwords))])\n",
        "    verdict = input()\n",
        "    \n",
        "    if verdict == 'y':\n",
        "        count += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starring role\n",
            "y\n",
            "Album of the Year\n",
            "y\n",
            "trying to steal\n",
            "n\n",
            "Conservative and Unionist\n",
            "y\n",
            "Faustian ideal\n",
            "y\n",
            "soundtrack for the film was released\n",
            "y\n",
            "Robert Cray\n",
            "y\n",
            "Detroit Pistons\n",
            "y\n",
            "Marshgate Lane\n",
            "y\n",
            "session held on Saturday\n",
            "n\n",
            "Distinguished Flying Cross\n",
            "y\n",
            "earned run average\n",
            "n\n",
            "aims to build\n",
            "n\n",
            "Cape Canaveral\n",
            "y\n",
            "prize for Category\n",
            "y\n",
            "Sufi saint\n",
            "y\n",
            "many injuries\n",
            "n\n",
            "make sense\n",
            "y\n",
            "Narragansett Steamship\n",
            "y\n",
            "extremely close\n",
            "n\n",
            "eastern Europe\n",
            "y\n",
            "commuted into the municipality\n",
            "y\n",
            "Days' Battles\n",
            "y\n",
            "Cuthbert served in the Georgia\n",
            "n\n",
            "Malibu Comics\n",
            "y\n",
            "departing Pearl\n",
            "n\n",
            "west coast of Graham Land\n",
            "y\n",
            "digital downloads\n",
            "y\n",
            "abstract concepts\n",
            "y\n",
            "Je Khenpo\n",
            "y\n",
            "tremendous amount\n",
            "y\n",
            "illusory palinopsia\n",
            "y\n",
            "trains per hour\n",
            "n\n",
            "southern terminus\n",
            "y\n",
            "already been in use\n",
            "n\n",
            "released two albums\n",
            "n\n",
            "Jerry Lawler\n",
            "y\n",
            "Copa Libertadores\n",
            "y\n",
            "court to order\n",
            "n\n",
            "Light Horse Regiment\n",
            "y\n",
            "distribution center\n",
            "y\n",
            "debut for Yeovil\n",
            "y\n",
            "Russian armies\n",
            "y\n",
            "forested land\n",
            "y\n",
            "fifth round\n",
            "n\n",
            "eWorld service\n",
            "y\n",
            "Cuthbert served\n",
            "n\n",
            "Associated Newspapers\n",
            "y\n",
            "Kerman Province\n",
            "y\n",
            "Maxwell Evarts\n",
            "y\n",
            "Intercollegiate Athletics\n",
            "y\n",
            "woredas in the Oromia\n",
            "y\n",
            "production of chlorine\n",
            "y\n",
            "content management systems\n",
            "y\n",
            "track listings\n",
            "y\n",
            "Hoodoo Gurus\n",
            "y\n",
            "Transitional Authority in Cambodia\n",
            "y\n",
            "Grapico Bottling\n",
            "y\n",
            "ITF Women's Circuit\n",
            "y\n",
            "Lonsdale Force\n",
            "y\n",
            "specifications called for\n",
            "n\n",
            "Hijiri Kuwano\n",
            "y\n",
            "Albanian being second most common\n",
            "n\n",
            "UC Berkeley\n",
            "y\n",
            "Belle Vue\n",
            "y\n",
            "Cross Country Championships\n",
            "y\n",
            "Three Kingdoms\n",
            "y\n",
            "Spiro T\n",
            "y\n",
            "cutting the deficit\n",
            "n\n",
            "Dutch Revolt\n",
            "y\n",
            "Iron Curtain\n",
            "y\n",
            "parent plant\n",
            "n\n",
            "part of an attempt\n",
            "n\n",
            "Louisiana State University in Baton Rouge\n",
            "y\n",
            "emigrated to the United States\n",
            "y\n",
            "Justice Department\n",
            "y\n",
            "Originally located\n",
            "n\n",
            "reductive elimination\n",
            "y\n",
            "big leagues\n",
            "y\n",
            "third party candidate\n",
            "y\n",
            "Irish Parliament\n",
            "y\n",
            "didn't receive any votes\n",
            "n\n",
            "Enoch Powell\n",
            "y\n",
            "lower incisors\n",
            "y\n",
            "Court for the Northern\n",
            "y\n",
            "Van Impe\n",
            "y\n",
            "BCR Open Romania\n",
            "y\n",
            "Reserve Command and Air\n",
            "y\n",
            "raised to the peerage as Baron\n",
            "n\n",
            "per capita income for the county\n",
            "y\n",
            "India's Ministry\n",
            "y\n",
            "arrest him\n",
            "n\n",
            "Colonial Parkway\n",
            "y\n",
            "Quality Protocol\n",
            "y\n",
            "Sault Ste\n",
            "y\n",
            "Affordable Care\n",
            "y\n",
            "Companion of the Order\n",
            "y\n",
            "incompatibilism with respect\n",
            "y\n",
            "Despite the lack\n",
            "y\n",
            "deep watershed infarcts\n",
            "y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKOO7AzRf82E",
        "colab_type": "code",
        "outputId": "af9bd472-c18f-4891-dd59-2e3e4d701ec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "FP = 100 - count\n",
        "TP = count\n",
        "precision = TP / (FP + TP)\n",
        "print('Precision: {} %'.format(100 * precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 77.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4aFlumrgHad",
        "colab_type": "text"
      },
      "source": [
        "Taking into consideration 100 randomly chosen Relevant Expressions from the ones extracted by our algorithm from the whole 2 mln words corpus and rejecting those starting with a stopword, we can say that 23 of them were incorrectly classified as REs and as a result our method reached 77% of precision. This small adjustment indeed improved obtainted result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNHp3R0E-1cf",
        "colab_type": "text"
      },
      "source": [
        "### Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s36mMRv3hLHK",
        "colab_type": "text"
      },
      "source": [
        "To be able to check the recall measure value for our algorithm we attached to the 2 mln corpus the article from wikipedia website ([link](https://en.wikipedia.org/wiki/CD_Projekt)) which topic is known for us and we can better judge whether an extracted Relevant Expression is indeed relevant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtXeeTp_XtJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open(\"/content/drive/My Drive/STUDIA/SEM 8/pad/en1.8mMY.txt\", \"r\", encoding=\"utf8\")\n",
        "\n",
        "whole_text = file.read()\n",
        "text = whole_text.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1qzVqKFiPqY",
        "colab_type": "text"
      },
      "source": [
        "Below, two first paragraphs of the article on which we will evaluate recall measure are presented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKUd6aBLZ-rK",
        "colab_type": "code",
        "outputId": "0637e3d1-e0bc-4616-87a6-272f7fa60c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "test = ''.join(text[-54:-51])\n",
        "pp.pprint(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('CD Projekt S.A. (Polish: [ˌt͡sɛˈdɛ ˈprɔjɛkt]) is a Polish video game '\n",
            " 'developer, publisher and distributor based in Warsaw, founded in May 1994 by '\n",
            " 'Marcin Iwiński and Michał Kiciński. Iwiński and Kiciński were video game '\n",
            " 'retailers before they founded the company, which initially acted as a '\n",
            " 'distributor of foreign video games for the domestic market. The department '\n",
            " 'responsible for developing original games, CD Projekt Red, best known for '\n",
            " 'The Witcher series, was formed in 2002. In 2008, CD Projekt launched the '\n",
            " 'digital distribution service GOG.com (originally as Good Old Games).The '\n",
            " 'company began by translating major Western video-game releases into Polish, '\n",
            " \"collaborating with Interplay Entertainment for two Baldur's Gate games. CD \"\n",
            " \"Projekt was working on the PC version of Baldur's Gate: Dark Alliance when \"\n",
            " 'Interplay experienced financial difficulties. The game was cancelled and the '\n",
            " 'company decided to reuse the code for their own video game. It became The '\n",
            " 'Witcher, a video game based on the works of Andrzej Sapkowski.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_tvW7RBYcT0",
        "colab_type": "code",
        "outputId": "617bcacc-ef20-49e7-862a-130171ac1d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "RE = main(text, 7, 1, SCP_f, True)\n",
        "print(\"Obtained\", len(RE), \"Relevant Expressions\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing text\n",
            "Building dictionary with frequencies\n",
            "Proceeding to calculating glue value\n",
            "Glue value calculated for:\n",
            "2 grams\n",
            "3 grams\n",
            "4 grams\n",
            "5 grams\n",
            "6 grams\n",
            "7 grams\n",
            "Proceeding to Localmax\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm_notebook.py:88: TqdmExperimentalWarning: Detect Google Colab 0.0.1a2 and thus load dummy ipywidgets package. Note that UI is different from that in Jupyter. See https://github.com/tqdm/tqdm/pull/640\n",
            "  \" See https://github.com/tqdm/tqdm/pull/640\".format(colab.__version__), TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='69522' value='69522'></progress>100% 69522/69522 [01:00&lt;00:00, 1152.88it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Obtained 19089 Relevant Expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0W_5rsyZY10",
        "colab_type": "code",
        "outputId": "e3ac2f03-6cdb-4e9d-e7dd-da7ac85b2f73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "RE[-30:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['momentary afterimages',\n",
              " 'Floriano de Azevedo',\n",
              " 'Floriano de Azevedo Marques Neto',\n",
              " 'Azevedo Marques Neto',\n",
              " 'Jammu Tawi',\n",
              " 'Rinse FM',\n",
              " 'Table tennis',\n",
              " 'Edith Cavell',\n",
              " 'Jadavpur University',\n",
              " \"Ernest Hemingway's\",\n",
              " 'scientific payload',\n",
              " 'short supply',\n",
              " 'Summer League team',\n",
              " 'Throughout gameplay',\n",
              " 'and Teacher of',\n",
              " 'CD Projekt',\n",
              " 'Witcher series',\n",
              " 'Good Old Games',\n",
              " 'Interplay Entertainment',\n",
              " \"Baldur's Gate\",\n",
              " 'Interplay experienced financial',\n",
              " 'console port',\n",
              " 'Assassins of Kings',\n",
              " 'help players find',\n",
              " \"Dark Alliance's cancellation\",\n",
              " \"Alliance's cancellation\",\n",
              " \"Projekt Red's\",\n",
              " \"CD Projekt's\",\n",
              " 'decided to focus on',\n",
              " 'iOS and Android']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z1CVIXti1Gn",
        "colab_type": "text"
      },
      "source": [
        "Above the Relevant Expressions extracted from the article are presented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTOGvfytg4Va",
        "colab_type": "code",
        "outputId": "57ad2155-847c-453c-dee2-0bba3ce7e893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "FN = 3 # digital distribution service, dark alliance, andrzej sapkowski\n",
        "TP = 5\n",
        "recall = TP / (FN + TP)\n",
        "print('Recall: {} %'.format(100 * recall))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall: 62.5 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiECsaDfBmIb",
        "colab_type": "text"
      },
      "source": [
        "The value of recall measure was calculated based on our judgement about Relevant Expressions contained by the first 2 paragraphs of the article attached to the corpus. The achieved result is quite satisfying (62.5%). In our opinion, for example 'Dark Alliance' or 'Andrzej Sapkowski' should be cosidered as Relevant Expressions instead of \"Dark Alliance's cancellation\" or \"Alliance's cancellation\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXsqKyjCGSVy",
        "colab_type": "text"
      },
      "source": [
        "###  F-metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64yqMSEuGV7v",
        "colab_type": "code",
        "outputId": "3581b951-f891-46e9-ab0f-f1557966b31f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "F1 = 2 * (precision * recall) / (precision + recall)\n",
        "print('F-metric: {} %'.format(100 * round(F1, 3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F-metric: 69.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP2jM5OcHQ81",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the value of F-metric is equal 69%."
      ]
    }
  ]
}